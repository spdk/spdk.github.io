---
layout: post
title:  "Benchmarking Intel&reg; Optane&trade; SSD DC P4800X"
author: Ben Walker & Vishal Verma
categories: benchmark
---

A number of great reviews of the newly released Intel&reg; Optane&trade; SSD DC P4800X were released last week, including one from [AnandTech](http://www.anandtech.com/show/11209/intel-optane-ssd-dc-p4800x-review-a-deep-dive-into-3d-xpoint-enterprise-performance) and another from [Tom's Hardware](http://www.tomshardware.com/reviews/intel-optane-3d-xpoint-p4800x,5030.html). The benchmark results were primarily produced using fio 2.19 on top of recent Linux kernels. We'd like to take this opportunity to provide some complementary benchmarks using SPDK.

# Configuration

We're going to focus on 4KiB random reads and writes for this post because there's plenty of material for those benchmarks alone. In a future post we'll do more comprehensive testing with SPDK for a wider variety of I/O sizes and queue depths. This testing will be performed using fio 2.19 on the following system:

|            | Intel&reg; Optane&trade; DC P4800X Test Platform |
|------------|--------------------------------------------------|
| Processors | 2x Intel&reg; Xeon&reg; E5-2699 v3 @ 2.3 GHz (3.6 GHz Turbo) |
| Memory     | 12x 4GiB DDR4-1866                    |
| SSD    | Intel&reg; SSD DC P4800X 375GB, AIC, PCIe 3.0 x4      |
| OS         | Fedora† Linux 25. Kernel 4.10.10-200.fc25.x86_64  |
| SPDK       | 9aaccfe |
| DPDK       | 17.02 |
{: .table .table-striped}

The base fio configuration file is available at the end of this blog post. This should allow anyone to reproduce these numbers.

With a device capable of performing I/O so quickly there is necessarily a lot of complex software tuning. The Linux kernel we're using is set up to poll NVMe devices for completions. Hybrid polling is disabled because the latency is better with continual polling. We'll investigate the advantages of hybrid polling at another time. All of the CPUs are placed in performance mode as well. The device was preconditioned by first deallocating all blocks, then by doing two full drive writes sequentially.

As far as configuring fio, there are three reasonable choices for the backend ioengine: libaio, pvsync2, and SPDK's fio_plugin. The libaio backend is built on top of Linux AIO, so it submits I/O asynchronously and polls for completions. It actually does not busy poll and our benchmarks show about 25% to 33% of one core used by the application itself. Note that it's the userspace application polling - the kernel happens to also be configured to poll for completions and so burns 100% of a core while I/O is outstanding in addition to the application usage. The pvsync2 engine submits I/O using preadv2 and pwritev2, which are blocking calls. The kernel appears to poll in the same thread context that the calls were made from, so these test runs also consume 100% of a single core. Note that the pvsync2 engine can't do more than 1 queue depth per thread, so to actually ramp up the queue depth would require more threads (which would also burn 100% of their cores). SPDK's fio_plugin is fully asynchronous and polls for completions. It consumes 100% of one CPU core, but can submit any queue depth requested. For each run we use the same fio configuration file (at bottom of this post), and we manually pass `--ioengine` and `--filename` on the command line. For the pvsync2 backend, we also specify the `--hipri` option.

# Queue Depth 1

We've gone on enough about benchmarking methodology, so let's get to the numbers.

<table class="table table-striped">
        <thead>
                <tr>
                        <th>IOPS</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>87.5k</td>
                        <td>108k</td>
                        <td class="success">138k</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>93.4k</td>
                        <td>122k</td>
                        <td class="success">150k</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>Mean Latency</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>11.04 μs</td>
                        <td>9.09 μs</td>
                        <td class="success">7.03 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>10.35 μs</td>
                        <td>8.02 μs</td>
                        <td class="success">6.45 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99th Percentile Latency</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>36 μs</td>
                        <td>10 μs</td>
                        <td class="success">9 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>20 μs</td>
                        <td>10 μs</td>
                        <td class="success">9 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99.999th Percentile Latency</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>105 μs</td>
                        <td class="success">77 μs</td>
                        <td>83 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>113 μs</td>
                        <td>37 μs</td>
                        <td class="success">35 μs</td>
                </tr>
        </tbody>
</table>

Let's start with the obvious: **SPDK is 1.5 to 2 μs faster for both read and write.** SPDK is purely a software change - the hardware is the same - so that savings is gained entirely by eliminating software overhead. It's not as if SPDK is burning more CPU cycles here either; the kernel is burning a core on polling too.

Polling also removes a considerable amount of jitter from all paths, but there appears to be significant additional software-induced jitter when using the kernel. This shows up most prominently at the 99th percentile with libaio. Again, the hardware here has been held constant, so that additional variation is all software effects. At the 99.999th percentile it's harder to analyze - the range here really could be hardware effects and much longer runs would be required to make these numbers consistent.

The final and less obvious take-away is that we're hitting the very limits of what fio can do. The results are so sensitive to the amount of software overhead that each additional metric tracked by fio has a big impact on the performance. We're open to suggestions regarding our fio configuration file to try and further reduce overhead. For instance, we've tried eliminating the `gettimeofday` calls, but then we can't directly measure latency, so it's a trade off. The SPDK project includes an NVMe performance test example (called "perf") written to minimize the impact of measurement overhead and expose the performance of the media with the thinnest possible software layer. These benchmarks are all taken with fio to show a fair comparison between SPDK and the other backends, but the perf example is very slightly faster.

# Queue Depth 4

Let's check out how this scales up to higher queue depths. Note that we're still limiting ourselves to a **single thread**. The pvsync2 backend can't submit more than one I/O per thread due to the blocking nature of the POSIX API it is using, so it has been removed below.


<table class="table table-striped">
        <thead>
                <tr>
                        <th>IOPS</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>276k</td>
                        <td class="success">435k</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>282k</td>
                        <td class="success">520k</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>Mean Latency</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>14.14 μs</td>
                        <td class="success">8.97 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>13.82 μs</td>
                        <td class="success">7.48 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99th Percentile Latency</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>51 μs</td>
                        <td class="success">47 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>40 μs</td>
                        <td class="success">18 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99.999th Percentile Latency</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>127 μs</td>
                        <td class="success">114 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>94 μs</td>
                        <td class="success">66 μs</td>
                </tr>
        </tbody>
</table>

The SPDK backend scales up very nicely. 520k 4KiB random read IOPS using only a single core is a really incredible number and it's just a bit shy of the maximum performance that the device is capable of at any queue depth. It's very difficult to find an SSD of any size and at any queue depth that can hit that number, let alone a 375 GiB SSD at queue depth 4. **Note that the mean latency with SPDK at queue depth 4 is lower than the kernel at queue depth 1.**

The benchmarks on [Tom's Hardware](http://www.tomshardware.com/reviews/intel-optane-3d-xpoint-p4800x,5030-5.html) and [AnandTech](http://www.anandtech.com/show/11209/intel-optane-ssd-dc-p4800x-review-a-deep-dive-into-3d-xpoint-enterprise-performance/5) both show about 300k random write IOPS at queue depth 4. They don't give details as to which backend they are using or how many individual threads or jobs they've specified, but their numbers are not far off from what we're measuring using libaio and a single thread. The difference may simply be that they are testing on a system with one generation more recent of a CPU.

# Saturation Point

The rule of thumb being reported so far is that the device is expected to hit maximum IOPS around queue depth 8 for reads and queue depth 16 for writes. However, those measurements were all taken using the Linux kernel, and saturation points are often measured in only powers of two queue depths. Let's see if SPDK can saturate the device at lower queue depths, walking up the depth by only 1 each time.

<table class="table table-striped">
        <thead>
                <tr>
                        <th>Work Load</th>
                        <th>QD 4</th>
                        <th>QD 5</th>
                        <th>QD 6</th>
                        <th>QD 7</th>
                        <th>QD 8</th>
                        <th>QD 9</th>
                        <th>QD 10</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>428k</td>
                        <td>491k</td>
                        <td>541k</td>
                        <td>547k</td>
                        <td>553k</td>
                        <td class="success">559k</td>
                        <td>539k</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>513k</td>
                        <td>562k</td>
                        <td class="success">583k</td>
                        <td>581k</td>
                        <td>574k</td>
                        <td>573k</td>
                        <td>575k</td>
                </tr>
        </tbody>
</table>

The device hits its saturation point for writes at queue depth 9 and for reads at queue depth 6!

# FIO Configuration File

Below is the fio configuration file that was used in these tests for the queue depth 1 measurements. The ioengine and filename were passed in on the command line. To test other queue depths, the file was modified only to change the iodepth parameter.

```
[global]
thread=1
group_reporting=1
direct=1
verify=0
norandommap=1
cpumask=1
percentile_list=50:99:99.9:99.99:99.999

[precondition-trim]
description="Trim the entire device"
rw=trim
iodepth=128
bs=1M

[precondition-sequential]
stonewall
description="Sequentially write to the device twice"
rw=write
iodepth=128
bs=128k
loops=2

[4k_randwrite_qd1]
stonewall
description="4KiB Random Write QD=1"
bs=4k
rw=randwrite
iodepth=1
time_based=1
ramp_time=60
runtime=240

[4k_randread_qd1]
stonewall
description="4KiB Random Read QD=1"
bs=4k
rw=randread
iodepth=1
time_based=1
ramp_time=60
runtime=240
```

† Other names and brands may be claimed as the property of others
