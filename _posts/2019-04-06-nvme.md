---
layout: post
title:  10M 4KiB Random Reads Per Second From One Thread
author: Ben Walker
categories: news
---

Yes, you're reading that right. That's *10 MILLION* 4KiB random read I/O
operations per second on a single thread. And to top it off, the average latency
is 65us. Mind blown? Don't believe it? This blog post will explain exactly how
we've done it.

## Introduction

For those of you joining us from outside the SPDK development community, SPDK is
a set of user space C libraries that re-imagine how a block storage stack should
operate. It contains much of the same functionality you'd find in a traditional
operating system block storage stack, from device drivers all the way up to
logical volume management. SPDK was designed from the beginning to support what
we call "next generation" storage media, such as Intel Optane(tm) devices.
Designing for low latency and high throughput is what this project is all about.

The following will explain the techniques and strategies that SPDK employs to
achieve what we feel are mind-bogglingly great performance numbers. We're going
to start the series by focusing on the lowest level first - the user space NVMe
driver. We'll show some benchmarks showing SPDK performing more than 10 million
4KiB random read I/O per second, on real hardware available today, using just a
single thread, all while keeping the average latency per I/O to just *[LATENCY NUMBER]us*.
Later posts will move up the block stack, exploring the performance of SPDK when
using the generic block storage API, logical volumes, and ultimately over the
network with NVMe-oF or to a virtual machine using vhost.

What you'll find here is (hopefully) deeply technical. We're going to be
discussing very low level hardware concepts - the CPU cache, PCIe, MMIO, Direct
I/O, etc. - and how they drive our software designs. So grab a cup of coffee and
settle in.

## BIG NUMBERS

We love big numbers, so let's start with those:

BENCHMARK RESULTS HERE

Here's the specs for the system we ran this on. It's expensive, but note that
it's composed of entirely commercial, off the shelf products. It's a basically
regular Xeon server platform with a bunch of NVMe SSDs plugged into it.

SYSTEM SPECS HERE

## The NVMe Specification

Before we can talk shop on how SPDK drives I/O to NVMe devices so quickly, we
need to cover a bit about how an I/O flows to and from an NVMe device. NVMe
devices allow host software (in this case, the SPDK NVMe driver) to allocate
queue pairs in host memory. I'll use the term "host" a lot, so to clarify I'm
talking about the system that the NVMe SSD is plugged into. A queue pair
consists of two queues - a submission queue and a completion queue. These queues
are better described as circular rings of fixed size entries. The submission
queue is an array of 64 byte command structures, plus 2 integers (head and tail
indices). The completion queue is similarly an array of 16 byte completion
structures, plus 2 integers (head and tail indices). There are also two 32-bit
registers involved that are called doorbells.

An I/O is submitted to an NVMe device by constructing a 64 byte command, placing
it into the submission queue at the current location of the submission queue
head index, and then writing the new index of the submission queue head to the
submission queue doorbell register. It's actually valid to copy a whole set of
commands into open slots in the ring and then write the doorbell just one time
to submit the whole batch.

NEED GRAPHIC

The command itself describes the operation and also, if necessary, a location in
host memory containing a descriptor for host memory associated with the command.
This host memory is the data to be written on a write command, or the location
to place the data on a read command. Data is transferred to or from this
location using a DMA engine on the NVMe device.

The completion queue works similarly, but the device is instead the one writing
entries into the ring. Each entry contains a "phase" bit that toggles between 0
and 1 on each loop through the entire ring. When a queue pair is set up to
generate interrupts, the interrupt contains the index of the completion queue
head. However, SPDK doesn't enable interrupts and instead polls on this phase
bit to detect completions. Interrupts are very heavy operations, so polling this
phase bit is often far more efficient.

## The I/O Path

Let's now cover what the major components are in the SPDK NVMe driver and what
their relative costs are. The user is going to construct a queue pair at some
early time in the life cycle of the program, so that's not part of the "hot"
path. Then, they'll call functions like spdk_nvme_ns_cmd_read() to perform an
I/O operation. The user supplies a data buffer, the target LBA, and the length,
as well as other information like which NVMe namespace the command is targeted
at and which NVMe queue pair to use. Finally, the user provides a callback
function and context pointer that will be called when a completion for the
resulting command is discovered during a later call to
spdk_nvme_qpair_process_completions().

The first stage in the driver is allocating a request object to track the operation. The
operations are asynchronous, so we can't simply track the state of the request
on the call stack. Allocating a new request object on the heap would be far too
slow, so SPDK keeps a pre-allocated set of request objects inside of the NVMe
queue pair object - struct spdk_nvme_qpair. The number of requests allocated to
the queue pair is larger than the actual queue depth of the NVMe submission
queue because SPDK supports a couple of key convenience features. The first is
software queueing - SPDK will allow the user to submit more requests than the
hardware queue can actually hold and SPDK will automatically queue in software.
The second is splitting - SPDK will split a request for many reasons, some of
which are outlined next. The number of request objects is configurable at queue
pair creation time and if not specified, SPDK will pick a sensible number based
on the hardware queue depth.

The second stage is building the 64 byte NVMe command itself. The command is
built into memory embedded into the request object itself - not directly into an
NVMe submission queue slot. Part of building the command is processing the data
buffer into a PRP list. That's essentially an NVMe scatter gather list, although
they are a bit more restricted. The user provides SPDK with the virtual address
of the buffer, so SPDK has to go do a page table look up to find the physical
address (pa) or I/O virtual addresses (iova) backing that virtual memory. A
virtually contiguous memory region may not be physically contiguous, so this may
result in a PRP list with multiple elements. Sometimes this may result in a set
of physical addresses that can't actually be expressed as a single PRP list, so
SPDK will automatically split the user operation into two separate requests
transparently.

Fortunately, the most common scenario is that the user supplies a data buffer
that is physically contiguous, which gets translated into a PRP list containing
a single element. Profiling shows that this section of the code is not a major
contributor to the overall CPU use.

Once the command has been constructed, SPDK attempts to obtain an open slot in
the NVMe submission queue. For each element in the submission queue an object
called a tracker is allocated. The trackers are allocated in an array, so they
can be quickly looked up by an index. The tracker itself contains a pointer to
the request currently occupying that slot. When a particular tracker is
obtained, the command's CID value is updated with the index of the tracker. The
NVMe specification provides that CID value in the completion, so the request can
be recovered by looking up the tracker via the CID value and then following the
pointer.

NEED PICTURE HERE

With a tracker in hand, SPDK copies the 64 byte command into the actual NVMe
submission queue slot and then rings the submission queue tail doorbell to tell
the device to go process it. SPDK then returns back to the user, without waiting
for a completion.

The user can periodically call spdk_nvme_qpair_process_completions() to tell
SPDK to examine the completion queue. Specifically, it reads the phase bit of
the next expected completion slot and when it flips, looks at the CID value to
find the tracker, which points at the request object. The request object
contains a function pointer that the user provided initially, which is then
called to complete the command.

The spdk_nvme_qpair_process_completions() function will keep advancing to the
next completion slot until it runs out of completions, at which point it will
write the completion queue head doorbell to let the device know that it can use
the completion queue slots for new completions.

## So How Do We Make This Fast?

Whew, we did it. We're all caught up on how the I/O path works, so now we can
get to the fun stuff. There are really four key things for performance:

1. No cross-thread coordination (locks, etc.).
2. Poll instead of interrupt.
3. Minimize MMIO.
4. Get the important things into the CPU cache at the right time.

SPDK, by design, assigns NVMe queue pairs to threads. Because NVMe has multiple
independent submission queues, we can get away with this. That solves problem 1
above entirely, so we won't spend much time discussing that. It's important to
note, however, that operating system drivers can't assign queue pairs to
threads, because the lifetime of the operating system driver is much longer than
any given thread and there may be many more threads on the system than available
queue pairs. So operating systems typically assign NVMe queue pairs to CPU cores
and take locks to coordinate with the threads on that CPU core. This minimizes
lock contention considerably, but doesn't eliminate it. SPDK, as a user space
application, is able to directly assign queue pairs to threads.

## Don't Interrupt Me!

This is the oldest trick in the SPDK arsenal, and is quickly spreading to all of
the major operating systems as well. The main issue is that handling an
interrupt is very expensive because it requires the CPU to stop whatever it was
doing, swap out the stack, swap in the interrupt handler, execute it, swap it
back out, and then resume what it was doing. That's not a fast operation, and
it's not friendly to the CPU cache. When storage devices completed an I/O every
couple of milliseconds, this was a great choice because it let the CPUs go idle
while waiting. However, with an I/O completing every microsecond or less, we
can't stop to handle an interrupt any more.

So instead SPDK completes all I/O by polling. Specifically, the user of the NVMe
driver has to explicitly call spdk_nvme_qpair_process_completions() to check for
completed I/O.

## Tricks for minimizing MMIO

Each doorbell ring, both on the submission and the completion side, is an MMIO
write. MMIO writes are posted, meaning the CPU does not stall waiting for any
acknowledgement that the write made it to the PCIe device, so they're much
faster than MMIO reads in general. That said, they should still be avoided as
much as possible.

When the CPU performs an MMIO write, a request is generated that's placed into a
hardware queue to later be sent over the PCI bus to the device. This queue can
vary in size based on the specifics of the platform - generally server platforms
have a much deeper queue. If the CPU generates too many MMIO and overflows the
queue, the CPU will stall and wait for a slot at the end of the queue to open
up. If the driver were to perform an MMIO on each command submission and on each
command completion, it would be capped at just a couple million I/O per second
on most platforms.

The first strategy we implemented to minimize MMIO was done inside the function
spdk_nvme_qpair_process_completions(). That function walks through the
completion queue entries in an NVMe queue pair's completion queue and checks for
entries whose phase bit has flipped. For each entry found, we need to update the
completion queue head doorbell. What we do here is wait to write the doorbell
until we've built up the whole set of outstanding completions for this function
call, then ring it at the end. This is a fairly standard practice - almost every
NVMe driver does this today.

However, SPDK can do much better by taking advantage of the fact that it's
polling. By setting an option on the NVMe queue pair, a user can indicate that
they only want to ring doorbells inside of calls to
spdk_nvme_qpair_process_completions(). Then, the user can call the various I/O
submission functions, such as spdk_nvme_ns_cmd_read(), multiple times and then
finally poll for completions and ring the doorbell. The I/O submission
functions, in this case, are just building the command into the submission queue
ring and returning. If a user is submitting I/O at queue depth 8, this reduces
the number of MMIO writes on submission by a factor of 8. If the user is
submitting I/O at a queue depth of 32, this reduces the number of MMIO writes on
the submission side by a factor of 32, etc. This yields enormous performance
improvements - we saw up to 2x jumps in our benchmarking.

The final and most advanced way to avoid MMIO occurs on the completion side. We
have batched completion queue doorbell head writes within a single poll call
from the beginning, but it turns out that we can delay that doorbell write
considerably longer. In fact, we don't actually need to tell the device that we
consumed completion queue entries until the device needs some to become
available, and the SPDK driver can easily calculate when that condition occurs.
For example, if the completion queue ring has 1024 entries and the user submits
32 commands and they all complete, we don't need to tell the device that we've
processed those 32 completion entries. There are still 992 available for it to
use anyway. As more commands are submitted, SPDK can simply do the math until it
sees that the newly submitted commands won't have a completion queue entry
available for them, and only then ring the doorbell. In practice, we go until
that condition or until the completion queue ring has been half-way consumed.
This reduces completion queue MMIO to near zero - one per 512 commands or so for
a ring with 1024 total entries.

With all of these techniques combined, SPDK performs far fewer MMIO writes than
most NVMe drivers and this is one of the largest contributors to SPDK's high
performance.

## Maximizing The Impact of the CPU Cache

Small code is fast code because it fits in cache. To set the stage, the CPU has
three layers of caches - L1, L2, and L3. Each level gets successively bigger,
but takes longer to access. L1 cache is further split into two parts, an
instruction cache and a data cache. The goal is to have the right things in the
L1 data cache at the right time.

Any time data is accessed, the CPU pulls in an entire cache line into the CPU
cache. Cache lines are 64 bytes on the CPUs we're focusing on. Since the cache
is limited in size, pulling in a cache line typically requires evicting another
one that we'll probably need later. We spend a lot of time trying to eliminate
cache line thrashing like this.

There are a couple of tricks in the SPDK NVMe driver we're employing to help
here. The most basic technique is structure packing, where we rearrange the data
layout of the structs to try and get as much data into each cache line as
possible. We make extensive use of the 'pahole' utility from the dwarves package
to look at our data structures, find extra padding inserted by the compiler, and
rearrange to make sure all of the holes are filled. Rearranging is better than
instructing the compiler to pack the structure because rearranging maintains
data alignment.

PICTURE OF PAHOLE OUTPUT HERE. MAYBE AN EXAMPLE?
Caption: pahole is best named tool in the history of tools

However, we often get considerably more crafty than that. For example, most
code in a device driver is written to handle corner cases such as errors. We
intentionally arrange our data structures so that all of the data touched in the
normal operation path (hot path) sits on the same cache lines, whereas data only
accessed during an error (cold path) is moved elsewhere. That results in fewer
cache line accesses overall, which means more of the important data stays in the
cache.

In addition to hot path/cold path separation, we'll also often separate data by
submission and completion path. An I/O submission is processed from entry point
down until it is submitted at the device, but that I/O is completed at some
later time. Often, we can have a small number of common cache lines accessed
in both paths, and then a submission cache line and a completion cache line
that are separate. This again results in less thrashing.

## Dependent Loads

Modern CPUs rely heavily on speculation to achieve high performance. They do
this by looking ahead many instructions to start loading the correct data into
the CPU cache, hoping that the data has arrived in the cache by the time the
instruction needs to be executed. It's critical that applications structure
their loads in such a way that this speculation does not get stalled. Here's
some real code (simplified) that existed in SPDK up until a few weeks ago:

```c
struct nvme_request {
	spdk_nvme_cmd_cb cb_fn; /* Callback function */
	void *cb_arg;
};

struct nvme_tracker {
	struct nvme_request *req;

	/* Other stuff */

	uint8_t padding[...]; /* The prp entry begins on the second cache line of the tracker */

	uint64_t prp[NVME_MAX_PRP_LIST_ENTRIES];
};
```

The NVMe completion queue is an array of completion queue entries. Inside those
entries is a CID value that SPDK provided on command submission. SPDK allocates
an array of tracker objects where the index is this CID. Remember, SPDK allows
users to queue up more requests than there are actual slots in the submission
queue, and that NVMe allows commands to complete in any order after submission.
Further, the tracker objects are what contain the PRP list, which is basically
an NVMe scatter gather list. So trackers are much larger than requests and must
be allocated from special memory so that the device can DMA this scatter gather
list directly. This necessitates this three level hierarchy of structures.

 When checking for completions, SPDK would do the following:

```c
if (cqe->phase == phase_flipped) {
	struct tracker *tr = tracker_array[cqe->cid];

	struct request *req = tr->req;

	req->cb_fn(req->cb_arg);
}
```

If we imagine ourselves speculating ahead to figure out all of the loads
necessary for this function, it's something like this:

1. Load cqe to read phase bit and CID.
2. Load tracker - Depends on knowing CID to calculate the address of the tracker (array offset).
3. Load request - Must have loaded tracker to get pointer to request.
4. Call user callback function - Must have request loaded.

We can see that the CPU effectively cannot speculate ahead on the loads here -
it has to do them in serial. Luckily, the tracker objects had a bit of extra
space in their first cache line before the prp list started. So we simply
changed the struct definitions to the following:

```c
struct nvme_request {
	spdk_nvme_cmd_cb cb_fn; /* Callback function */
	void *cb_arg;
};

struct nvme_tracker {
	struct nvme_request *req;

	spdk_nvme_cmd_cb cb_fn; /* Callback function */
	void *cb_arg;

	/* Other stuff */

	uint64_t prp[NVME_MAX_PRP_LIST_ENTRIES];
};
```

On request submission into an actual slot, we copy the cb_fn and cb_arg into the
tracker. This is cheap because we just built the request object and set up the
tracker anyway, so they're in cache at that point. Then on the completion side
the CPU is able to speculate a bit better:

1. Load cqe to read phase bit and CID.
2. Load tracker - Depends on knowing CID to calculate the address of the tracker (array offset).
3. Load request - Must have loaded tracker to get pointer to request.
4. Call user callback function - Must have **tracker** loaded.

The key is step 4, which now can happen in parallel to step 3. Often, several
checks are done inside that user callback prior to actually accessing any data
in the request, so it masks the latency of that request load entirely. When a
stack is as highly optimized as SPDK, these seemlingly small things can have a
very large impact. This change, for example, resulted in a 500k I/O per second
improvement on a single thread.

## Leveraging Non-Temporal Writes

For each command we first build the command into a temporary space in our
request structure, then we copy it into the command submission queue slot when
one becomes available. In this scheme, the CPU never ends up reading any of the
memory in the submission queue at all - it only writes it and the device later
does a DMA from that location. Since the CPU never reads that memory, it doesn't
benefit much from the CPU cache.

Quanitfying the benefit of the CPU cache in this scenario is a bit tricky
though. To perform the copy into the submission queue ring we use one of a
couple different strategies, depending on available instructions. The simplest
one is the following:

```c
uint64_t *d64 = (uint64_t *)dst;
uint64_t *s64 = (const uin64_t *)src;

d64[0] = s64[0];
d64[1] = s64[1];
d64[2] = s64[2];
d64[3] = s64[3];
d64[4] = s64[4];
d64[5] = s64[5];
d64[6] = s64[6];
d64[7] = s64[7];
```

That's 8 pairs of 64 bit load and mov instructions, which is the naive
implementation of an assignment generated by the compiler. If the platform has
SSE2 or AVX, we'll instead use those. The code is the following:

```c
#if defined(__AVX512F__)
	__m512i *d512 = (__m512i *)dst;
	const __m512i *s512 = (const __m512i *)src;

	_mm512_store_si512(d512, _mm512_load_si512(s512));
#elif defined(__AVX__)
	__m256i *d256 = (__m256i *)dst;
	const __m256i *s256 = (const __m256i *)src;

	_mm256_store_si256(&d256[0], _mm256_load_si256(&s256[0]));
	_mm256_store_si256(&d256[1], _mm256_load_si256(&s256[1]));
#elif defined(__SSE2__)
	__m128i *d128 = (__m128i *)dst;
	const __m128i *s128 = (const __m128i *)src;

	_mm_store_si128(&d128[0], _mm_load_si128(&s128[0]));
	_mm_store_si128(&d128[1], _mm_load_si128(&s128[1]));
	_mm_store_si128(&d128[2], _mm_load_si128(&s128[2]));
	_mm_store_si128(&d128[3], _mm_load_si128(&s128[3]));
#else
	*dst = *src;
#endif
```

That's either 4 pairs of 128 bit operations, 2 pairs of 256 operations, or 1 512
bit operation. Typically, fewer operations are faster (although not always!).

Most importantly, that store instruction is going to write the data into a cache
line. To perform that update, it is going to pull that cache line into the L1
data cache for the CPU core executing the instruction. The load is also going to
pull the source cache line in, but since we just built the command at that
location it's probably sitting in the L1 data cache already.

The data can be written into the cache more quickly than it could be written to
main memory, but in the case where the cache line is not already present in the
L1 data cache, reading the cache line into L1 to do a partial update (in all
cases but AVX-512) negates that benefit. The device itself can also perform a
DMA directly out of the CPU cache instead of from main memory, which may be
marginally faster, although benchmarking shows the net performance delta for the
device DMA is negligible.

Given the above, what we really want to do is perform the stores in a way that
allows us to bypass the CPU cache and go directly to main memory, to avoid
evicting other likely useful data. Fortunately, the x86 instruction set has a
set of commands for performing what's called a "non-temporal" write, which is a
write to memory with a hint to bypass the CPU cache - exactly what we want.
Here's the code:

```c
#if defined(__AVX512F__)
	__m512i *d512 = (__m512i *)dst;
	const __m512i *s512 = (const __m512i *)src;

	_mm512_stream_si512(d512, _mm512_load_si512(s512));
#elif defined(__AVX__)
	__m256i *d256 = (__m256i *)dst;
	const __m256i *s256 = (const __m256i *)src;

	_mm256_stream_si256(&d256[0], _mm256_load_si256(&s256[0]));
	_mm256_stream_si256(&d256[1], _mm256_load_si256(&s256[1]));
#elif defined(__SSE2__)
	__m128i *d128 = (__m128i *)dst;
	const __m128i *s128 = (const __m128i *)src;

	_mm_stream_si128(&d128[0], _mm_load_si128(&s128[0]));
	_mm_stream_si128(&d128[1], _mm_load_si128(&s128[1]));
	_mm_stream_si128(&d128[2], _mm_load_si128(&s128[2]));
	_mm_stream_si128(&d128[3], _mm_load_si128(&s128[3]));
#else
	*dst = *src;
#endif
}
```

These instructions can broadcast the 64 byte cache line update without
allocating a cache line in the CPU's local L1D cache, which hopefully keeps
other valuable data in the L1D for later.

## Wrapping Up

