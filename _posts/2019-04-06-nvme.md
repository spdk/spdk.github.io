---
layout: post
title:  10M 4KiB Random Reads Per Second From One Thread
author: Ben Walker
categories: news
---

## Introduction

The following blog post is the start of a series of posts on the low-level
strategies SPDK uses to optimize performance. For those of you joining us from
outside the SPDK development community, SPDK is a set of user space C libraries
that reimagine how a block storage stack should operate. It contains much of the
same functionality you'd find in a traditional operating system block storage
stack, from device drivers all the way up to logical volume management. SPDK was
designed from the beginning to support what we call "next generation" storage
media, such as Intel Optane(tm) devices. Designing for low latency and high
throughput is what this project is all about.

This post is the first in a series of posts that will walk through the
techniques and strategies that SPDK employs to achieve what we feel are
mind-bogglingly great performance numbers. We're going to focus on the lowest
level first - the user space NVMe driver. In this post will be some benchmarks
showing SPDK performing more than 10 million 4KiB random read I/O per second, on
real hardware available today, using just a single thread, all while
keeping the average latency per I/O to just *[LATENCY NUMBER]us*. Later posts
will move up the block stack, exploring the performance of SPDK when using the
generic block storage API, logical volumes, and ultimately over the network with
NVMe-oF or to a virtual machine using vhost.

What you'll find here is (hopefully) deeply technical. We're going to be
discussing very low level hardware concepts - the CPU cache, PCIe, MMIO, Direct
I/O, and more. So let's get to it.

## BIG NUMBERS

We love big numbers, so let's start with those:

BENCHMARK RESULTS HERE

Yes, you're reading that right. That's *10 MILLION* 4KiB random I/O
operations per second on a single thread. Here's the specs for the system we ran
this on. It's expensive, but note that it's composed of entirely commercial, off
the shelf products, without anything fancy. It's a regular Xeon server platform
with a bunch of NVMe SSDs plugged into it.

SYSTEM SPECS HERE

## The NVMe Specification

Before we can talk shop on how SPDK drives I/O to NVMe devices so quickly, we
need to cover a bit about how an I/O flows to and from an NVMe device. NVMe
devices allow host software (in this case, the SPDK NVMe driver) to allocate
queue pairs in host memory. I'll use the term "host" a lot, so to clarify I'm
talking about the system that the NVMe SSD is plugged into. A queue pair
consists of two queues - a submission queue and a completion queue. These queues
are better described as circular rings of fixed size entries. The submission
queue is an array of 64 byte command structures, plus 2 integers (head and tail
indices). The completion queue is similarly an array of 16 byte completion
structures, plus 2 integers (head and tail indices). There are also two 32-bit
registers involved that are called doorbells.

An I/O is submitted to an NVMe device by constructing a 64 byte command, placing
it into the submission queue at the current location of the submission queue
head index, and then writing the new index of the submission queue head to the
submission queue doorbell register. It's actually valid to copy a whole set of
commands into open slots in the ring and then write the doorbell just one time
to submit the whole batch.

NEED GRAPHIC

The command itself describes the operation and also, if necessary, a location in
host memory containing a descriptor for host memory associated with the command.
This host memory is the data to be written on a write command, or the location
to place the data on a read command. Data is transferred to or from this
location using a DMA engine on the NVMe device.

The completion queue works similarly, but the device is instead the one writing
entries into the array. Each entry contains a "phase" bit that toggles between 0
and 1 on each loop through the entire ring. When a queue pair is set up to
generate interrupts, the interrupt contains the index of the completion queue
head. However, SPDK doesn't enable interrupts and instead polls on this phase
bit to detect completions. Interrupts are very heavy operations, so polling this
phase bit is often far more efficient.

## The I/O Path

Let's start by going over what the major components are in the driver and what
their relative costs are. The user is going to construct a queue pair at some
early time in the life cycle of the program, so that's not part of the "hot"
path. Then, they'll call functions like spdk_nvme_ns_cmd_read() to perform an
operation. The user supplies a data buffer, the target LBA, and the length, as
well as other information like which NVMe namespace and which NVMe queue pair.
Finally, the user provides a callback function and context pointer that will be
called when a completion for the resulting command is discovered during a call
to spdk_nvme_qpair_process_completions().

The first stage is allocating a request object to track the operation. The
operations are asynchronous, so we can't simply track the state of the request
on the call stack. Allocating a new request object on the heap would be far too
slow, so SPDK keeps a pre-allocated set of request objects inside of the NVMe
queue pair object - struct spdk_nvme_qpair. The number of requests allocated to
the queue pair is larger than the actual queue depth of the NVMe submission
queue because SPDK supports a couple of key convenience features. The first is
software queueing - SPDK will allow the user to submit more requests than the
hardware queue can actually hold and SPDK will automatically queue in software.
The second is splitting - SPDK will split a request for many reasons, some of
which are outlined next. The number of request objects is configurable at queue
pair creation time and if not specified, SPDK will pick a sensible number based
on the hardware queue depth.

The second stage is building the 64 byte NVMe command itself. The command is
built into memory embedded into the request object itself - not directly into an
NVMe submission queue slot. Part of building the command is processing the data
buffer into a PRP list. That's essentially an NVMe scatter gather list, although
they are a bit more restricted. The user provides SPDK with the virtual address
of the buffer, so SPDK has to go do a page table look up to find the physical
address (pa) or I/O virtual addresses (iova) backing that virtual memory. A
virtually contiguous memory region may not be physically contiguous, so this may
result in a PRP list with multiple elements. Sometimes this may result in a set
of physical addresses that can't actually be expressed as a single PRP list, so
SPDK will automatically split the user operation into two separate requests
transparently.

Fortunately, the most common scenario is that the user supplies a data buffer
that is physically contiguous, which gets translated into a PRP list containing
a single element. Profiling shows that this section of the code is not a major
contributor to the overall CPU use.

Once the command has been constructed, SPDK attempts to obtain an open slot in
the NVMe submission queue. For each element in the submission queue an object
called a tracker is allocated. The trackers are allocated in an array, so they
can be quickly looked up by an index. The tracker itself contains a pointer to
the request currently occupying that slot. When a particular tracker is
obtained, the command's CID value is updated with the index of the tracker. The
NVMe specification provides that CID value in the completion, so the request can
be recovered by looking up the tracker via the CID value and then following the
pointer.

NEED PICTURE HERE

With a tracker in hand, SPDK copies the 64 byte command into the actual NVMe
submission queue slot and then rings the submission queue tail doorbell to tell
the device to go process it. SPDK then returns back to the user, without waiting
for a completion.

The user can periodically call spdk_nvme_qpair_process_completions() to tell
SPDK to examine the completion queue. Specifically, it reads the phase bit of
the next expected completion slot and when it flips, looks at the CID value to
find the tracker, which points at the request object. The request object
contains a function pointer that the user provided initially, which is then
called to complete the command.

The spdk_nvme_qpair_process_completions() function will keep advancing to the
next completion slot until it runs out of completions, at which point it will
write the completion queue head doorbell to let the device know that it can use
the completion queue slots for new completions.

## So How Do We Make This Fast?

Whew, we did it. We're all caught up on how the I/O path works, so now we can
get to the fun stuff. There are really four key things for performance:

1. No cross-thread coordination (locks, etc.).
2. Poll instead of interrupt.
3. Minimize MMIO.
4. Get the important things into the CPU cache at the right time.

SPDK, by design, assigns NVMe queue pairs to threads. Because NVMe has multiple
independent submission queues, we can get away with this. That solves problem 1
above entirely, so we won't spend much time discussing that. It's important to
note, however, that operating system drivers can't assign queue pairs to
threads, because the lifetime of the operating system driver is much longer than
any given thread and there may be many more threads on the system than available
queue pairs. So operating systems typically assign NVMe queue pairs to CPU cores
and take locks or other steps to coordinate with the threads on that CPU core.
This minimizes locking considerably, but doesn't eliminate it. SPDK, as a user
space application, is able to directly assign queue pairs to threads.

## Don't Interrupt Me!

This is the oldest trick in the SPDK arsenal, and is quickly spreading to all of
the major operating systems as well. The main issue is that handling an
interrupt is very expensive because it requires the CPU to stop whatever it was
doing, swap out the stack, swap in the interrupt handler, execute it, swap it
back out, and then resume what it was doing. That's not a fast operation, and
it's not friendly to the CPU cache. When storage devices completed an I/O every
couple of milliseconds, this was a great choice because it let the CPUs go idle
while waiting. However, with an I/O completing every microsecond or less, we
can't stop to handle an interrupt any more.

SPDK completes all I/O, then, by polling. The user of the NVMe driver has to
explicitly call spdk_nvme_qpair_process_completions() to check for completed
I/O.

## Tricks for minimizing MMIO

Each doorbell ring, both on the submission and the completion side, is an MMIO
write. MMIO writes are posted, meaning the CPU does not stall waiting for any
acknowledgement that the write made it to the PCIe device, so they're much
faster than MMIO reads in general. That said, they should still be avoided as
much as possible.

When the CPU performs an MMIO write, a request is generated that's placed into a
hardware queue to later be sent over the PCI bus to the device. This queue can
vary in size based on the specifics of the platform - generally server platforms
have a much deeper queue. If the CPU generates too many MMIO and overflows the
queue, the CPU will stall and wait for a slot at the end of the queue to open
up. If the driver were to perform an MMIO on each command submission and on each
command completion, it would be capped at just a couple million I/O per second
on most platforms.

The first strategy we implemented to minimize MMIO was done inside the function
spdk_nvme_qpair_process_completions(). That function walks through the
completion queue entries in an NVMe queue pair's completion queue and checks for
entries whose phase bit has flipped. For each entry found, we need to update the
completion queue head doorbell. What we do here is wait to write the doorbell
until we've built up the whole set of outstanding completions for this function
call, then ring it at the end. This is a fairly standard practice - almost every
NVMe driver does this today.

However, SPDK can do much better by taking advantage of the fact that it's
polling. By setting an option on the NVMe queue pair, a user can indicate that
they only wan to ring doorbells inside of calls to
spdk_nvme_qpair_process_completions(). Then, the user can call the various I/O
submission functions, such as spdk_nvme_ns_cmd_read() multiple times, then
finally poll for completions and ring the doorbell. The I/O submission
functions, in this case, are just building the command into the submission queue
ring and returning. If a user is submitting I/O at queue depth 8, this reduces
the number of MMIO writes on submission by a factor of 8. If the user is
submitting I/O at a queue depth of 32, this reduces the number of MMIO writes on
the submission side by a factor of 32, etc. This yields enormous performance
improvements - we saw up to 2x jumps in our benchmarking.

The final and most advanced way to avoid MMIO occurs on the completion side. We
have batched completion queue doorbell head writes within a single poll call
from the beginning, but it turns out that we can delay that doorbell write
considerably longer. In fact, we don't actually need to tell the device that we
consumed completion queue entries until the device needs some to become
available, and the SPDK driver can easily calculate when it needs some to become
available. For example, if the completion queue ring has 1024 entries and the
user submits 32 commands and they all complete, we don't need to tell the device
that we've processed those 32 entries. There are still 992 available for it to
use anyway. As more commands are submitted, SPDK can simply do the math until it
sees that the newly submitted commands won't have a completion queue entry
available for them, and only then ring the doorbell. In practice, we go until
that condition or until the completion queue ring has been half-way consumed.
This reduces completion queue MMIO to near zero - one per 512 commands or so.

## Maximizing The Impact of the CPU Cache

Small code is fast code because it fits in cache. The CPU has three layers of
caches - L1, L2, and L3. Each level gets successively bigger, but takes longer
to access. L1 cache is further split into two parts, an instruction cache and a
data cache. The goal is to have the right things in the L1 data cache at the
right time.

Any time data is accessed, the CPU pulls in an entire cache line into the CPU
cache. Cache lines are 64 bytes on the CPUs we're focusing on. Since the cache
is limited in size, pulling in a cache line typically requires evicting another
one that we'll probably need later. We spend a lot of time trying to eliminate
cache line thrashing like this.

There are a couple of tricks in the SPDK NVMe driver we're employing to help
here. The most basic technique is structure packing, where we rearrange the data
layout of the structs to try and get as much data into each cache line as
possible. We make extensive use of the 'pahole' utility from the dwarves package
to look at our data structures, find extra padding inserted by the compiler, and
rearrange to make sure all of the holes are filled. Rearranging is better than
instructing the compiler to pack the structure because rearranging maintains
data alignment.

PICTURE OF PAHOLE OUTPUT HERE. MAYBE AN EXAMPLE?
Caption: pahole is best named tool in the history of tools

However, we often get considerably more crafty than that. For example, most
code in a device driver is written to handle corner cases such as errors. We
intentionally arrange our data structures so that all of the data touched in the
normal operation path (hot path) sits on the same cache lines, whereas data only
accessed during an error (cold path) is moved elsewhere. That results in fewer
cache line accesses overall, which means more of the important data stays in the
cache.

In addition to hot path/cold path separation, we'll also often separate data by
submission and completion path. An I/O submission is processed from entry point
down until it is submitted at the device, but that I/O is completed at some
later time. Often, we can have a small number of common cache lines accessed
in both paths, and then a submission cache line and a completion cache line
that are separate. This again results in less thrashing.

## Dependent Loads

## Pre-fetching

## Leveraging Non-Temporal Writes

As described earlier, for each command we first build the command into a
temporary space in our request structure, then we copy it into the command
submission queue slot when one becomes available. In this scheme, the CPU never
ends up reading any of the memory in the submission queue at all - it only
writes it and the device later does a DMA from that location. That means there's
no reason for that data to enter the CPU cache.

Fortunately, the x86 instruction set has a set of commands for performing what's
called a "non-temporal" write, which is a write to memory with a hint to bypass
the CPU cache. SPDK performs its copy using non-temporal instructions on x86.
Specifically, it uses this function:

```c
static inline void
nvme_pcie_copy_command(struct spdk_nvme_cmd *dst, const struct spdk_nvme_cmd *src)
{
	/* dst and src are known to be non-overlapping and 64-byte aligned. */
#if defined(__AVX__)
	__m256i *d256 = (__m256i *)dst;
	const __m256i *s256 = (const __m256i *)src;

	_mm256_stream_si256(&d256[0], _mm256_load_si256(&s256[0]));
	_mm256_stream_si256(&d256[1], _mm256_load_si256(&s256[1]));
#elif defined(__SSE2__)
	__m128i *d128 = (__m128i *)dst;
	const __m128i *s128 = (const __m128i *)src;

	_mm_stream_si128(&d128[0], _mm_load_si128(&s128[0]));
	_mm_stream_si128(&d128[1], _mm_load_si128(&s128[1]));
	_mm_stream_si128(&d128[2], _mm_load_si128(&s128[2]));
	_mm_stream_si128(&d128[3], _mm_load_si128(&s128[3]));
#else
	*dst = *src;
#endif
}
```

These are compiler instrinsics that translate to the
```vmovntdq``` instruction in the AVX case or the ```movntdq``` instruction in
the SSE2 case. This avoids pulling the submission queue entries into the CPU
cache and evicting some data that we'll almost certainly need later.

## Wrapping Up
