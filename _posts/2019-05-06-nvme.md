---
layout: post
title:  10M 4KiB Random Reads Per Second From One Thread
author: Ben Walker
categories: news
---

Yes, you're reading that right. That's *10 MILLION* 4KiB random read I/O
operations per second on a single thread. And to top it off, the average latency
is 65us. Mind blown? Don't believe it? This blog post will explain exactly how
we've done it.

## Introduction

For those of you joining us from outside the SPDK development community, SPDK is
a set of user space C libraries that re-imagine how a block storage stack should
operate. It contains much of the same functionality you'd find in a traditional
operating system block storage stack, from device drivers all the way up to
logical volume management. SPDK was designed from the beginning to support what
we call "next generation" storage media, such as Intel Optane(tm) devices.
Designing for low latency and high throughput is what this project is all about.
There is a lot of reading material over at SPDK's main documentation page [https://spdk.io/doc](https://spdk.io/doc).
In particular, check out the `Introduction` and `Concepts` sections.

The following will explain the techniques and strategies that SPDK employs to
achieve what we feel are mind-bogglingly great performance numbers. We're going
to start the series by focusing on the lowest level first - the user space NVMe
driver. We'll show some benchmarks showing SPDK performing more than 10 million
4KiB random read I/O per second, on real hardware available today, using just a
single thread, all while keeping the average latency per I/O to just *[LATENCY NUMBER]us*.
Later posts will move up the block stack, exploring the performance of SPDK when
using the generic block storage API, logical volumes, and ultimately over the
network with NVMe-oF or to a virtual machine using vhost.

What you'll find here is (hopefully) deeply technical. We're going to be
discussing very low level hardware concepts - the CPU cache, PCIe, MMIO, Direct
I/O, etc. - and how they drive our software designs. So grab a cup of coffee and
settle in.

## BIG NUMBERS

We love big numbers, so let's start with those:

![Performance Result](../../../../../img/blog/big_numbers.png "Over 9000!"){: .text-center }


Here's the specs for the system we ran this on. It's expensive, but note that
it's composed of entirely commercial, off the shelf products. It's a basically
regular Xeon server platform with a bunch of NVMe SSDs plugged into it.

SYSTEM SPECS HERE

## So How'd We Do It?

Now for the fun part! We're going to dive into every facet of exactly why SPDK
is able to hit this number. However, if you're not already very familiar with
how I/O is submitted to an NVMe device, you may want to start here: [Submitting I/O to an NVMe Device](https://spdk.io/doc/nvme_spec.html).

There are really four key things for performance:

1. No cross-thread coordination (locks, etc.).
2. Poll instead of interrupt.
3. Minimize MMIO.
4. Get the important things into the CPU cache at the right time.

SPDK, by design, assigns NVMe queue pairs to threads. Because NVMe has multiple
independent submission queues, we can get away with this. That solves problem 1
above entirely, so we won't spend much time discussing that. It's important to
note, however, that operating system drivers can't assign queue pairs to
threads, because the lifetime of the operating system driver is much longer than
any given thread and there may be many more threads on the system than available
queue pairs. So operating systems typically assign NVMe queue pairs to CPU cores
and take locks to coordinate with the threads on that CPU core. This minimizes
lock contention considerably, but doesn't eliminate it. SPDK, as a user space
application, is able to directly assign queue pairs to threads.

## Don't Interrupt Me!

This is the oldest trick in the SPDK arsenal, and is quickly spreading to all of
the major operating systems as well. The main issue is that handling an
interrupt is very expensive because it requires the CPU to stop whatever it was
doing, swap out the stack, swap in the interrupt handler, execute it, swap it
back out, and then resume what it was doing. That's not a fast operation, and
it's not friendly to the CPU cache. When storage devices completed an I/O every
couple of milliseconds, this was a great choice because it let the CPUs go idle
while waiting. However, with an I/O completing every microsecond or less, we
can't stop to handle an interrupt any more.

So instead SPDK completes all I/O by polling. Specifically, the user of the NVMe
driver has to explicitly call spdk_nvme_qpair_process_completions() to check for
completed I/O.

## Tricks for minimizing MMIO

Each doorbell ring, both on the submission and the completion side, is an MMIO
write. MMIO writes are posted, meaning the CPU does not stall waiting for any
acknowledgement that the write made it to the PCIe device, so they're much
faster than MMIO reads in general. That said, they should still be avoided as
much as possible.

When the CPU performs an MMIO write, a request is generated that's placed into a
hardware queue to later be sent over the PCI bus to the device. This queue can
vary in size based on the specifics of the platform - generally server platforms
have a much deeper queue. If the CPU generates too many MMIO and overflows the
queue, the CPU will stall and wait for a slot at the end of the queue to open
up. If the driver were to perform an MMIO on each command submission and on each
command completion, it would be capped at just a couple million I/O per second
on most platforms.

The first strategy we implemented to minimize MMIO was done inside the function
spdk_nvme_qpair_process_completions(). That function walks through the
completion queue entries in an NVMe queue pair's completion queue and checks for
entries whose phase bit has flipped. For each entry found, we need to update the
completion queue head doorbell. What we do here is wait to write the doorbell
until we've built up the whole set of outstanding completions for this function
call, then ring it at the end. This is a fairly standard practice - almost every
NVMe driver does this today.

However, SPDK can do much better by taking advantage of the fact that it's
polling. By setting an option on the NVMe queue pair, a user can indicate that
they only want to ring doorbells inside of calls to
spdk_nvme_qpair_process_completions(). Then, the user can call the various I/O
submission functions, such as spdk_nvme_ns_cmd_read(), multiple times and then
finally poll for completions and ring the doorbell. The I/O submission
functions, in this case, are just building the command into the submission queue
ring and returning. If a user is submitting I/O at queue depth 8, this reduces
the number of MMIO writes on submission by a factor of 8. If the user is
submitting I/O at a queue depth of 32, this reduces the number of MMIO writes on
the submission side by a factor of 32, etc. This yields enormous performance
improvements - we saw up to 2x jumps in our benchmarking.

The final and most advanced way to avoid MMIO occurs on the completion side. We
have batched completion queue doorbell head writes within a single poll call
from the beginning, but it turns out that we can delay that doorbell write
considerably longer. In fact, we don't actually need to tell the device that we
consumed completion queue entries until the device needs some to become
available, and the SPDK driver can easily calculate when that condition occurs.
For example, if the completion queue ring has 1024 entries and the user submits
32 commands and they all complete, we don't need to tell the device that we've
processed those 32 completion entries. There are still 992 available for it to
use anyway. As more commands are submitted, SPDK can simply do the math until it
sees that the newly submitted commands won't have a completion queue entry
available for them, and only then ring the doorbell. In practice, we go until
that condition or until the completion queue ring has been half-way consumed.
This reduces completion queue MMIO to near zero - one per 512 commands or so for
a ring with 1024 total entries.

With all of these techniques combined, SPDK performs far fewer MMIO writes than
most NVMe drivers and this is one of the largest contributors to SPDK's high
performance.

## Maximizing The Impact of the CPU Cache

Small code is fast code because it fits in cache. To set the stage, the CPU has
three layers of caches - L1, L2, and L3. Each level gets successively bigger,
but takes longer to access. L1 cache is further split into two parts, an
instruction cache and a data cache. The goal is to have the right things in the
L1 data cache at the right time.

Any time data is accessed, the CPU pulls in an entire cache line into the CPU
cache. Cache lines are 64 bytes on the CPUs we're focusing on. Since the cache
is limited in size, pulling in a cache line typically requires evicting another
one that we'll probably need later. We spend a lot of time trying to eliminate
cache line thrashing like this.

There are a couple of tricks in the SPDK NVMe driver we're employing to help
here. The most basic technique is structure packing, where we rearrange the data
layout of the structs to try and get as much data into each cache line as
possible. We make extensive use of the 'pahole' utility from the dwarves package
to look at our data structures, find extra padding inserted by the compiler, and
rearrange to make sure all of the holes are filled. Rearranging is better than
instructing the compiler to pack the structure because rearranging maintains
data alignment.

![Structure Packing](../../../../../img/blog/pahole.png "pahole is best named tool in the history of tools"){: .text-center }

However, we often get considerably more crafty than that. For example, most
code in a device driver is written to handle corner cases such as errors. We
intentionally arrange our data structures so that all of the data touched in the
normal operation path (hot path) sits on the same cache lines, whereas data only
accessed during an error (cold path) is moved elsewhere. That results in fewer
cache line accesses overall, which means more of the important data stays in the
cache.

In addition to hot path/cold path separation, we'll also often separate data by
submission and completion path. An I/O submission is processed from entry point
down until it is submitted at the device, but that I/O is completed at some
later time. Often, we can have a small number of common cache lines accessed
in both paths, and then a submission cache line and a completion cache line
that are separate. This again results in less thrashing.

## Dependent Loads

Modern CPUs rely heavily on speculation to achieve high performance. They do
this by looking ahead many instructions to start loading the correct data into
the CPU cache, hoping that the data has arrived in the cache by the time the
instruction needs to be executed. It's critical that applications structure
their loads in such a way that this speculation does not get stalled. Here's
some real code (simplified) that existed in SPDK up until a few weeks ago:

```c
struct nvme_request {
	spdk_nvme_cmd_cb cb_fn; /* Callback function */
	void *cb_arg;
};

struct nvme_tracker {
	struct nvme_request *req;

	/* Other stuff */

	uint8_t padding[...]; /* The prp entry begins on the second cache line of the tracker */

	uint64_t prp[NVME_MAX_PRP_LIST_ENTRIES];
};
```

The NVMe completion queue is an array of completion queue entries. Inside those
entries is a CID value that SPDK provided on command submission. SPDK allocates
an array of tracker objects where the index is this CID. Remember, SPDK allows
users to queue up more requests than there are actual slots in the submission
queue, and that NVMe allows commands to complete in any order after submission.
Further, the tracker objects are what contain the PRP list, which is basically
an NVMe scatter gather list. So trackers are much larger than requests and must
be allocated from special memory so that the device can DMA this scatter gather
list directly. This necessitates this three level hierarchy of structures.

 When checking for completions, SPDK would do the following:

```c
if (cqe->phase == phase_flipped) {
	struct tracker *tr = tracker_array[cqe->cid];

	struct request *req = tr->req;

	req->cb_fn(req->cb_arg);
}
```

If we imagine ourselves speculating ahead to figure out all of the loads
necessary for this function, it's something like this:

1. Load cqe to read phase bit and CID.
2. Load tracker - Depends on knowing CID to calculate the address of the tracker (array offset).
3. Load request - Must have loaded tracker to get pointer to request.
4. Call user callback function - Must have request loaded.

We can see that the CPU effectively cannot speculate ahead on the loads here -
it has to do them in serial. Luckily, the tracker objects had a bit of extra
space in their first cache line before the prp list started. So we simply
changed the struct definitions to the following:

```c
struct nvme_request {
	spdk_nvme_cmd_cb cb_fn; /* Callback function */
	void *cb_arg;
};

struct nvme_tracker {
	struct nvme_request *req;

	spdk_nvme_cmd_cb cb_fn; /* Callback function */
	void *cb_arg;

	/* Other stuff */

	uint64_t prp[NVME_MAX_PRP_LIST_ENTRIES];
};
```

On request submission into an actual slot, we copy the cb_fn and cb_arg into the
tracker. This is cheap because we just built the request object and set up the
tracker anyway, so they're in cache at that point. Then on the completion side
the CPU is able to speculate a bit better:

1. Load cqe to read phase bit and CID.
2. Load tracker - Depends on knowing CID to calculate the address of the tracker (array offset).
3. Load request - Must have loaded tracker to get pointer to request.
4. Call user callback function - Must have **tracker** loaded.

The key is step 4, which now can happen in parallel to step 3. Often, several
checks are done inside that user callback prior to actually accessing any data
in the request, so it masks the latency of that request load entirely. When a
stack is as highly optimized as SPDK, these seemlingly small things can have a
very large impact. This change, for example, resulted in a 500k I/O per second
improvement on a single thread.

## Leveraging Non-Temporal Writes

For each command we first build the command into a temporary space in our
request structure, then we copy it into the command submission queue slot when
one becomes available. In this scheme, the CPU never ends up reading any of the
memory in the submission queue at all - it only writes it and the device later
does a DMA from that location. Since the CPU never reads that memory, it doesn't
benefit much from the CPU cache.

Quanitfying the benefit of the CPU cache in this scenario is a bit tricky
though. To perform the copy into the submission queue ring we use one of a
couple different strategies, depending on available instructions. The simplest
one is the following:

```c
uint64_t *d64 = (uint64_t *)dst;
uint64_t *s64 = (const uin64_t *)src;

d64[0] = s64[0];
d64[1] = s64[1];
d64[2] = s64[2];
d64[3] = s64[3];
d64[4] = s64[4];
d64[5] = s64[5];
d64[6] = s64[6];
d64[7] = s64[7];
```

That's 8 pairs of 64 bit load and mov instructions, which is the naive
implementation of an assignment generated by the compiler. If the platform has
SSE2 or AVX, we'll instead use those. The code is the following:

```c
#if defined(__AVX512F__)
	__m512i *d512 = (__m512i *)dst;
	const __m512i *s512 = (const __m512i *)src;

	_mm512_store_si512(d512, _mm512_load_si512(s512));
#elif defined(__AVX__)
	__m256i *d256 = (__m256i *)dst;
	const __m256i *s256 = (const __m256i *)src;

	_mm256_store_si256(&d256[0], _mm256_load_si256(&s256[0]));
	_mm256_store_si256(&d256[1], _mm256_load_si256(&s256[1]));
#elif defined(__SSE2__)
	__m128i *d128 = (__m128i *)dst;
	const __m128i *s128 = (const __m128i *)src;

	_mm_store_si128(&d128[0], _mm_load_si128(&s128[0]));
	_mm_store_si128(&d128[1], _mm_load_si128(&s128[1]));
	_mm_store_si128(&d128[2], _mm_load_si128(&s128[2]));
	_mm_store_si128(&d128[3], _mm_load_si128(&s128[3]));
#else
	*dst = *src;
#endif
```

That's either 4 pairs of 128 bit operations, 2 pairs of 256 operations, or 1 512
bit operation. Typically, fewer operations are faster (although not always!).

Most importantly, that store instruction is going to write the data into a cache
line. To perform that update, it is going to pull that cache line into the L1
data cache for the CPU core executing the instruction. The load is also going to
pull the source cache line in, but since we just built the command at that
location it's probably sitting in the L1 data cache already.

The data can be written into the cache more quickly than it could be written to
main memory, but in the case where the cache line is not already present in the
L1 data cache, reading the cache line into L1 to do a partial update (in all
cases but AVX-512) negates that benefit. The device itself can also perform a
DMA directly out of the CPU cache instead of from main memory, which may be
marginally faster, although benchmarking shows the net performance delta for the
device DMA is negligible.

Given the above, what we really want to do is perform the stores in a way that
allows us to bypass the CPU cache and go directly to main memory, to avoid
evicting other likely useful data. Fortunately, the x86 instruction set has a
set of commands for performing what's called a "non-temporal" write, which is a
write to memory with a hint to bypass the CPU cache - exactly what we want.
Here's the code:

```c
#if defined(__AVX512F__)
	__m512i *d512 = (__m512i *)dst;
	const __m512i *s512 = (const __m512i *)src;

	_mm512_stream_si512(d512, _mm512_load_si512(s512));
#elif defined(__AVX__)
	__m256i *d256 = (__m256i *)dst;
	const __m256i *s256 = (const __m256i *)src;

	_mm256_stream_si256(&d256[0], _mm256_load_si256(&s256[0]));
	_mm256_stream_si256(&d256[1], _mm256_load_si256(&s256[1]));
#elif defined(__SSE2__)
	__m128i *d128 = (__m128i *)dst;
	const __m128i *s128 = (const __m128i *)src;

	_mm_stream_si128(&d128[0], _mm_load_si128(&s128[0]));
	_mm_stream_si128(&d128[1], _mm_load_si128(&s128[1]));
	_mm_stream_si128(&d128[2], _mm_load_si128(&s128[2]));
	_mm_stream_si128(&d128[3], _mm_load_si128(&s128[3]));
#else
	*dst = *src;
#endif
}
```

These instructions can broadcast the 64 byte cache line update without
allocating a cache line in the CPU's local L1D cache, which hopefully keeps
other valuable data in the L1D for later.

## Wrapping Up

