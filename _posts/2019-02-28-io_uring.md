---
layout: post
title:  "io_uring - the Linux kernel's response to SPDK"
author: Jim Harris, Ben Walker, Vishal Verma
categories: release
---

SPDK started as a project at Intel in 2013, and was first open-sourced
in 2015.  While SPDK provides a wide variety of libraries for userspace
storage use cases, its first and most well-known library is the userspace
NVM Express (NVMe) driver.  Since the very beginning, performance of the
SPDK NVMe driver has been compared against the Linux kernel NVMe driver
using Linux AIO (or libaio).  Linux AIO has traditionally been the
most efficient and highest performing method for doing I/O from an
application using a Linux kernel-mode driver.

More recently, Jens Axboe has created a new Linux kernel interface called
[io_uring](https://lwn.net/Articles/776230/).  io_uring enables more
efficient NVMe (and other types of) I/O from applications, via several
techniques that are similar to SPDK:

- avoid interrupt overhead via use of dedicated NVMe queue pairs
- avoid system calls (as much as possible) by enabling I/O submission and polling for completions with a single system call
- avoid pinning overhead by enabling fixed I/O buffers

Jens has posted some initial
[performance data](https://lore.kernel.org/linux-block/20190116175003.17880-1-axboe@kernel.dk/)
indicating that on a per-CPU core basis, io_uring is virtually on par with
SPDK for NVMe I/O performance.  The SPDK team at Intel has done our
own benchmarking and analysis however, and come up with significantly
different results.  We also have some differing opinions on the best workloads
to perform these comparisons.  One thing is very clear - io_uring is
a huge improvement over Linux AIO.  SPDK will have support to use
io_uring as a storage backend for its storage networking and virtualization
frontends, so that users can perform their own comparisons.  But we also
believe SPDK's NVMe driver still maintains a 3-5x efficiency advantage over
io_uring, and this blog post will describe the basis for that advantage in
more detail.

Description of io_uring

- shortcomings (still requires system calls)
	- batching can help - but that's not best for storage workloads
- still "just" a backend storage driver - storage networking or storage
  virtualization still needs to be implemented in userspace

SPDK v. io_uring benchmarking

- fio v. SPDK nvme/perf (will need batching support) v. SPDK bdev
- SPDK io_uring bdev - enables io_uring devices to be used with SPDK frontend protocols
- configuration description

Performance data

- single Xeon core, 24 SSDs
- io_uring v. SPDK NVMe driver, with and without LTO/PGO (should do LTO/PGO even with SPDK io_uring)
- w/ NVMe-oF or vhost?
- SPDK lvols v. Linux LVM io_uring?

Summary (including links to SPDK io_uring patches)

- io_uring is a huge improvement over Linux aio
- io_uring can narrow gap v. SPDK to 2-3x but only with heavy batching
- for 'normal' workloads, SPDK still shows a 4-5x advantage
