---
layout: post
title:  "Benchmarking Intel&reg; Optane&trade; SSD DC P4800X"
author: Ben Walker & Vishal Verma
categories: benchmark
---

A number of great reviews of the newly released Intel&reg; Optane&trade; SSD DC
P4800X were released, including one from
[AnandTech](http://www.anandtech.com/show/11209/intel-optane-ssd-dc-p4800x-review-a-deep-dive-into-3d-xpoint-enterprise-performance)
and another from
[Tom's Hardware](http://www.tomshardware.com/reviews/intel-optane-3d-xpoint-p4800x,5030.html).
The benchmark results were primarily produced using fio 2.19 on top of recent
Linux kernels. We'd like to take this opportunity to provide some complementary
benchmarks using SPDK.

# Configuration

We're going to focus on 4KiB random reads and writes for this post because
there's plenty of material for those benchmarks alone. In a future post we'll
do more comprehensive testing with SPDK for a wider variety of I/O sizes and
queue depths. This testing will be performed using fio 3.3 on the following
system:

|            | Intel&reg; Optane&trade; DC P4800X Test Platform             |
|------------|--------------------------------------------------------------|
| Processors | 2x Intel&reg; Xeon&reg; E5-2699 v3 @ 2.3 GHz (3.6 GHz Turbo) |
| Memory     | 12x 4GiB DDR4-1866                                           |
| SSD        | Intel&reg; SSD DC P4800X 375GB, AIC, PCIe 3.0 x4             |
| OS         | Fedora† Linux 26. Kernel 4.14.8-200.fc26.x86_64              |
| SPDK       | 17.10.1                                                      |
| DPDK       | 17.08.0                                                      |
{: .table .table-striped}

With a device capable of performing I/O so quickly there is necessarily a lot
of complex software tuning. The Linux kernel we're using is set up to poll NVMe
devices for completions. Hybrid polling is disabled because the latency is
better with continual polling. We'll investigate the advantages of hybrid
polling at another time. All of the CPUs are placed in performance mode as
well, which makes them run near their maximum frequency. Futher, Turbo Boost is
disabled because we want to eliminate all sources of additional variability
outside of the SSD itself. A script to set up a system for these benchmarks is
available
[here](https://github.com/spdk/spdk/blob/master/scripts/prep_benchmarks.sh).

The device was preconditioned by first deallocating all blocks, then by doing
two full drive writes sequentially. The base fio configuration file is
available at the end of this blog post. This should allow anyone to reproduce
these numbers.

As far as configuring fio, there are three reasonable choices for the backend
ioengine: libaio, pvsync2, and SPDK's fio_plugin. The libaio backend is built
on top of Linux AIO, so it submits I/O asynchronously and polls for
completions. It actually does not busy poll and our benchmarks show about 25%
to 33% of one core used by the application itself. Note that it's the userspace
application polling - the kernel happens to also be configured to poll for
completions and so burns 100% of a core while I/O is outstanding in addition to
the application usage. The pvsync2 engine submits I/O using preadv2 and
pwritev2, which are blocking calls. The kernel appears to poll in the same
thread context that the calls were made from, so these test runs also consume
100% of a single core. Note that the pvsync2 engine can't do more than 1 queue
depth per thread, so to actually ramp up the queue depth would require more
threads (which would also burn 100% of their cores). SPDK's fio_plugin is fully
asynchronous and polls for completions. It consumes 100% of one CPU core, but
can submit any queue depth requested. For each run we use the same fio
configuration file (at bottom of this post), and we manually pass `--ioengine`
and `--filename` on the command line. For the pvsync2 backend, we also specify
the `--hipri` option.

# Queue Depth 1

We've gone on enough about benchmarking methodology, so let's get to the numbers.

<table class="table table-striped">
        <thead>
                <tr>
                        <th>IOPS</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>88.5k</td>
                        <td>108k</td>
                        <td class="success">137k</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>95.1k</td>
                        <td>120k</td>
                        <td class="success">151k</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>Mean Latency</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>10.93 μs</td>
                        <td>9.06 μs</td>
                        <td class="success">7.03 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>10.17 μs</td>
                        <td>8.13 μs</td>
                        <td class="success">6.38 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99th Percentile Latency</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>21.6 μs</td>
                        <td>20.4 μs</td>
                        <td class="success">8.51 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>19.6 μs</td>
                        <td>17.5 μs</td>
                        <td class="success">7.71 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99.999th Percentile Latency</th>
                        <th>libaio</th>
                        <th>pvsync2</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>85 μs</td>
                        <td>72 μs</td>
                        <td class="success">70 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>47 μs</td>
                        <td>46 μs</td>
                        <td class="success">38 μs</td>
                </tr>
        </tbody>
</table>

Let's start with the obvious: **SPDK is 1.5 to 2 μs faster for both read and
write.** SPDK is purely a software change - the hardware is the same - so that
savings is gained entirely by eliminating software overhead. It's not as if
SPDK is burning more CPU cycles here either; the kernel is burning a core on
polling too.

Polling also removes a considerable amount of jitter from all paths, but there
appears to be significant additional software-induced jitter when using the
kernel. This shows up most prominently at the 99th percentile with libaio.
Again, the hardware here has been held constant, so that additional variation
is all software effects. At the 99.999th percentile it's harder to analyze -
the range here really could be hardware effects and much longer runs would be
required to make these numbers consistent.

The final and less obvious take-away is that we're hitting the very limits of
what fio can do. The results are so sensitive to the amount of software
overhead that each additional metric tracked by fio has a big impact on the
performance. We're open to suggestions regarding our fio configuration file to
try and further reduce overhead. For instance, we've tried eliminating the
`gettimeofday` calls, but then we can't directly measure latency, so it's a
trade off. The SPDK project includes an NVMe performance test example (called
"perf") written to minimize the impact of measurement overhead and expose the
performance of the media with the thinnest possible software layer. These
benchmarks are all taken with fio to show a fair comparison between SPDK and
the other backends, but the perf example is very slightly faster.

# Queue Depth 4

Let's check out how this scales up to higher queue depths. Note that we're
still limiting ourselves to a **single thread**. The pvsync2 backend can't
submit more than one I/O per thread due to the blocking nature of the POSIX API
it is using, so it has been removed below.


<table class="table table-striped">
        <thead>
                <tr>
                        <th>IOPS</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>290k</td>
                        <td class="success">433k</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>308k</td>
                        <td class="success">534k</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>Mean Latency</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>13.42 μs</td>
                        <td class="success">9.00 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>12.69 μs</td>
                        <td class="success">7.26 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99th Percentile Latency</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>52 μs</td>
                        <td class="success">47 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>24 μs</td>
                        <td class="success">18 μs</td>
                </tr>
        </tbody>
        <thead>
                <tr>
                        <th>99.999th Percentile Latency</th>
                        <th>libaio</th>
                        <th>spdk</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>104 μs</td>
                        <td class="success">89 μs</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>89 μs</td>
                        <td class="success">52 μs</td>
                </tr>
        </tbody>
</table>

The SPDK backend scales up very nicely. 520k 4KiB random read IOPS using only a
single core is a really incredible number and it's just a bit shy of the
maximum performance that the device is capable of at any queue depth. It's very
difficult to find an SSD of any size and at any queue depth that can hit that
number, let alone a 375 GiB SSD at queue depth 4. **Note that the mean latency
with SPDK at queue depth 4 is lower than the kernel at queue depth 1.**

The benchmarks on [Tom's
Hardware](http://www.tomshardware.com/reviews/intel-optane-3d-xpoint-p4800x,5030-5.html)
and
[AnandTech](http://www.anandtech.com/show/11209/intel-optane-ssd-dc-p4800x-review-a-deep-dive-into-3d-xpoint-enterprise-performance/5)
both show about 300k random write IOPS at queue depth 4, so our numbers agree
with theirs.

# Saturation Point

The rule of thumb being reported so far is that the device is expected to hit
maximum IOPS around queue depth 8 for reads and queue depth 16 for writes.
However, those measurements were all taken using the Linux kernel, and
saturation points are often measured in only powers of two queue depths. Let's
see if SPDK can saturate the device at lower queue depths, walking up the depth
by only 1 each time.

<table class="table table-striped">
        <thead>
                <tr>
                        <th>Work Load</th>
                        <th>QD 4</th>
                        <th>QD 5</th>
                        <th>QD 6</th>
                        <th>QD 7</th>
                        <th>QD 8</th>
                        <th>QD 9</th>
                        <th>QD 10</th>
                </tr>
        </thead>
        <tbody>
                <tr>
                        <td>4KiB Random Write</td>
                        <td>428k</td>
                        <td>491k</td>
                        <td>541k</td>
                        <td>547k</td>
                        <td>553k</td>
                        <td class="success">559k</td>
                        <td>539k</td>
                </tr>
                <tr>
                        <td>4KiB Random Read</td>
                        <td>513k</td>
                        <td>562k</td>
                        <td class="success">583k</td>
                        <td>581k</td>
                        <td>574k</td>
                        <td>573k</td>
                        <td>575k</td>
                </tr>
        </tbody>
</table>

The device hits its saturation point for writes at queue depth 9 and for reads
at queue depth 6!

# FIO Configuration File

Below is the fio configuration file that was used in these tests for the queue
depth 1 measurements. The ioengine and filename were passed in on the command
line. To test other queue depths, the file was modified only to change the
iodepth parameter.

```
[global]
thread=1
group_reporting=1
direct=1
verify=0
norandommap=1
cpumask=1
disable_slat=1
disable_bw=1
lat_percentiles=1
clat_percentiles=0
percentile_list=50:99:99.999

[precondition-sequential]
stonewall
description="Sequentially write to the device"
rw=write
iodepth=128
bs=128k

[4k_randwrite_qd1]
stonewall
description="4KiB Random Write QD=1"
bs=4k
rw=randwrite
iodepth=1
time_based=1
ramp_time=10
runtime=240

[4k_randread_qd1]
stonewall
description="4KiB Random Read QD=1"
bs=4k
rw=randread
iodepth=1
time_based=1
ramp_time=10
runtime=240
```

† Other names and brands may be claimed as the property of others
