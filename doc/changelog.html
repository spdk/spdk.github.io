<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <!-- For Mobile Devices -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/xhtml; charset=utf-8">
  <meta name="generator" content="Doxygen 1.8.13">
  <title>SPDK: Changelog</title>
  <script type="text/javascript" src="jquery.js"></script>
  <script type="text/javascript" src="dynsections.js"></script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,900" type="text/css">
  <link href="../css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="tabs.css" type="text/css">
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div class="container-fluid">
  <div id="top">  <!-- do not remove this div, it is closed by doxygen! -->
    <div class="row no-gutters">
      <div class="col-sm-12">
        <section id="nav">
          <div class="navbar navbar-default navbar-static-top banner-tabs">
            <ul class="nav navbar-nav">
              <li role="presentation">
                <a href="http://www.spdk.io/">
                  <i class="glyphicon glyphicon-home"></i>
                  <span class="box-name">home</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/releases/">
                  <i class="glyphicon glyphicon-download-alt"></i>
                  <span class="box-name">download</span>
                </a>
              </li>
              <li class="active" role="presentation">
                <a href="index.html">
                  <i class="glyphicon glyphicon-book"></i>
                  <span class="box-name">documentation</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/development/">
                  <i class="glyphicon glyphicon-wrench"></i>
                  <span class="box-name">development</span>
                </a>
              </li>
              <li role="presentation">
                <a href="https://ci.spdk.io/status/">
                  <i class="glyphicon glyphicon-wrench"></i>
                  <span class="box-name">CI status</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/community/">
                  <i class="glyphicon glyphicon-envelope"></i>
                  <span class="box-name">community</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/blog/">
                  <i class="glyphicon glyphicon-comment"></i>
                  <span class="box-name">Blog</span>
                </a>
              </li>
              <li role="presentation">
                <a href="https://github.com/spdk/spdk/wiki/Roadmap">
                  <i class="glyphicon glyphicon-map-marker"></i>
                  <span class="box-name">Roadmap</span>
                </a>
              </li>
              <li role="presentation">
                <a href="https://github.com/spdk/spdk/wiki/News">
                  <i class="glyphicon glyphicon-bullhorn"></i>
                  <span class="box-name">News</span>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </div>
<!-- Generated by Doxygen 1.8.13 -->
</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Changelog </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="changelog-v17-10"></a>
v17.10: (Upcoming Release)</h1>
<h2>Block Device Abstraction Layer (bdev)</h2>
<p>An <a href="http://github.com/axboe/fio">fio</a> plugin was added that can route I/O to the bdev layer. See the https://github.com/spdk/spdk/blob/master/examples/bdev/fio_plugin/README.md "plugin documentation" for more information.</p>
<p><a class="el" href="bdev_8h.html#a680c9c302998f7b003e2476e35d9ae4b" title="Submit an unmap request to the block device. ">spdk_bdev_unmap()</a> was modified to take an offset and a length in bytes as arguments instead of requiring the user to provide an array of SCSI unmap descriptors. This limits unmaps to a single contiguous range.</p>
<p><a class="el" href="bdev_8h.html#adc50b78fec7f69190d9139aff29a9043" title="Submit a write zeroes request to the bdev on the given channel. ">spdk_bdev_write_zeroes()</a> was introduced as an alternative to <a class="el" href="bdev_8h.html#a680c9c302998f7b003e2476e35d9ae4b" title="Submit an unmap request to the block device. ">spdk_bdev_unmap()</a>. It ensures that all unmapped blocks will be zeroed out. This function is currently only supported by NVMe block devices.</p>
<h2>Linux AIO bdev</h2>
<p>The AIO bdev now allows the user to override the auto-detected block size.</p>
<h2>NVMe driver</h2>
<p>The NVMe driver now recognizes the NVMe 1.3 Namespace Optimal I/O Boundary field. NVMe 1.3 devices may report an optimal I/O boundary, which the driver will take into account when splitting I/O requests.</p>
<p>The HotplugEnable option in <code>[Nvme]</code> sections of the configuration file is now "No" by default. It was previously "Yes".</p>
<p>The NVMe library now includes a function spdk_nvme_ns_get_ctrlr which returns the NVMe Controller associated with a given namespace.</p>
<h2>NVMe-oF Target (nvmf)</h2>
<p>The NVMe-oF target no longer requires any in capsule data buffers to run, and the feature is now entirely optional. Previously, at least 4KiB in capsule data buffers were required.</p>
<h2>Environment Abstraction Layer</h2>
<p>A new default value, SPDK_MEMPOOL_DEFAULT_CACHE_SIZE, was added to provide additional clarity when constructing spdk_mempools. Previously, -1 could be passed and the library would choose a reasonable default, but this new value makes it explicit that the default is being used.</p>
<h2>Blobstore</h2>
<p>spdk_bs_io_readv_blob() and spdk_bs_io_writev_blob() were added to enable scattered payloads.</p>
<h1><a class="anchor" id="changelog-v17-07"></a>
v17.07: Build system improvements, userspace vhost-blk target, and GPT bdev</h1>
<h2>Build System</h2>
<p>A <code>configure</code> script has been added to simplify the build configuration process. The existing CONFIG file and <code>make CONFIG_...</code> options are also still supported. Run <code>./configure --help</code> for information about available configuration options.</p>
<p>A DPDK submodule has been added to make building SPDK easier. If no <code>--with-dpdk</code> option is specified to configure, the SPDK build system will automatically build a known-good configuration of DPDK with the minimal options enabled. See the Building section of README.md for more information.</p>
<p>A <a href="https://www.vagrantup.com/">Vagrant</a> setup has been added to make it easier to develop and use SPDK on systems without suitable NVMe hardware. See the Vagrant section of README.md for more information.</p>
<h2>Userspace vhost-blk target</h2>
<p>The vhost library and example app have been updated to support the vhost-blk protocol in addition to the existing vhost-scsi protocol. See the <a href="http://www.spdk.io/doc/vhost.html">vhost documentation</a> for more details.</p>
<h2>Block device abstraction layer (bdev)</h2>
<p>A GPT virtual block device has been added, which automatically exposes GPT partitions with a special SPDK-specific partition type as bdevs. See the <a href="http://www.spdk.io/doc/bdev.md#bdev_config_gpt">GPT bdev documentation</a> for more information.</p>
<h2>NVMe driver</h2>
<p>The NVMe driver has been updated to support recent Intel SSDs, including the Intel® Optane™ SSD DC P4800X series.</p>
<p>A workaround has been added for devices that failed to recognize register writes during controller reset.</p>
<p>The NVMe driver now allocates request tracking objects on a per-queue basis. The number of requests allowed on an I/O queue may be set during <code><a class="el" href="nvme_8h.html#a225bbc386ec518ae21bd5536f21db45d" title="Enumerate the bus indicated by the transport ID and attach the userspace NVMe driver to each device f...">spdk_nvme_probe()</a></code> by modifying <code>io_queue_requests</code> in the opts structure.</p>
<p>The SPDK NVMe <code>fio_plugin</code> has been updated to support multiple threads (<code>numjobs</code>).</p>
<p><a class="el" href="nvme_8h.html#a13f745d239dab9b8f934fae2ad4984a2" title="Allocate an I/O queue pair (submission and completion queue). ">spdk_nvme_ctrlr_alloc_io_qpair()</a> has been modified to allow the user to override controller-level options for each individual I/O queue pair. Existing callers with qprio == 0 can be updated to: </p><div class="fragment"><div class="line">... = spdk_nvme_ctrlr_alloc_io_qpair(ctrlr, NULL, 0);</div></div><!-- fragment --><p> Callers that need to specify a non-default qprio should be updated to: </p><div class="fragment"><div class="line">struct spdk_nvme_io_qpair_opts opts;</div><div class="line">spdk_nvme_ctrlr_get_default_io_qpair_opts(ctrlr, &amp;opts, sizeof(opts));</div><div class="line">opts.qprio = SPDK_NVME_QPRIO_...;</div><div class="line">... = spdk_nvme_ctrlr_alloc_io_qpair(ctrlr, &amp;opts, sizeof(opts));</div></div><!-- fragment --><h2>Environment Abstraction Layer</h2>
<p>The environment abstraction layer has been updated to include several new functions in order to wrap additional DPDK functionality. See <code><a class="el" href="env_8h.html" title="Encapsulated third-party dependencies. ">include/spdk/env.h</a></code> for the current set of functions.</p>
<h2>SPDK Performance Analysis with Intel® VTune™ Amplifier</h2>
<p>Support for SPDK performance analysis has been added to Intel® VTune™ Amplifier 2018.</p>
<p>This analysis provides:</p><ul>
<li>I/O performance monitoring (calculating standard I/O metrics like IOPS, throughput, etc.)</li>
<li>Tuning insights on the interplay of I/O and compute devices by estimating how many cores would be reasonable to provide for SPDK to keep up with a current storage workload.</li>
</ul>
<p>See the VTune Amplifier documentation for more information.</p>
<h1><a class="anchor" id="changelog-v17-03"></a>
v17.03: Blobstore and userspace vhost-scsi target</h1>
<h2>Blobstore and BlobFS</h2>
<p>The blobstore is a persistent, power-fail safe block allocator designed to be used as the local storage system backing a higher-level storage service. See the <a href="http://www.spdk.io/doc/blob.html">blobstore documentation</a> for more details.</p>
<p>BlobFS adds basic filesystem functionality like filenames on top of the blobstore. This release also includes a RocksDB Env implementation using BlobFS in place of the kernel filesystem. See the <a href="http://www.spdk.io/doc/blobfs.html">BlobFS documentation</a> for more details.</p>
<h2>Userspace vhost-scsi target</h2>
<p>A userspace implementation of the QEMU vhost-scsi protocol has been added. The vhost target is capable of exporting SPDK bdevs to QEMU-based VMs as virtio devices. See the <a href="http://www.spdk.io/doc/vhost.html">vhost documentation</a> for more details.</p>
<h2>Event framework</h2>
<p>The overhead of the main reactor event loop was reduced by optimizing the number of calls to <a class="el" href="env_8h.html#a7d868d272aa0c000f6135966dfadc596" title="Get a monotonic timestamp counter. ">spdk_get_ticks()</a> per iteration.</p>
<h2>NVMe library</h2>
<p>The NVMe library will now automatically split readv/writev requests with scatter-gather lists that do not map to valid PRP lists when the NVMe controller does not natively support SGLs.</p>
<p>The <code>identify</code> and <code>perf</code> NVMe examples were modified to add a consistent format for specifying remote NVMe over Fabrics devices via the <code>-r</code> option. This is implemented using the new <code><a class="el" href="nvme_8h.html#ac37484cc5d14777e4ae1fde031d0edf2" title="Parse the string representation of a transport ID. ">spdk_nvme_transport_id_parse()</a></code> function.</p>
<h2>iSCSI Target</h2>
<p>The [Nvme] section of the configuration file was modified to remove the <code>BDF</code> directive and replace it with a <code>TransportID</code> directive. Both local (PCIe) and remote (NVMe-oF) devices can now be specified as the backing block device. A script to generate an entire [Nvme] section based on the local NVMe devices attached was added at <code>scripts/gen_nvme.sh</code>.</p>
<h2>NVMe-oF Target</h2>
<p>The [Nvme] section of the configuration file was modified to remove the <code>BDF</code> directive and replace it with a <code>TransportID</code> directive. Both local (PCIe) and remote (NVMe-oF) devices can now be specified as the backing block device. A script to generate an entire [Nvme] section based on the local NVMe devices attached was added at <code>scripts/gen_nvme.sh</code>.</p>
<h1><a class="anchor" id="changelog-v16-12"></a>
v16.12: NVMe over Fabrics host, hotplug, and multi-process</h1>
<h2>NVMe library</h2>
<p>The NVMe library has been changed to create its own request memory pool rather than requiring the user to initialize the global <code>request_mempool</code> variable. Apps can be updated by simply removing the initialization of <code>request_mempool</code>. Since the NVMe library user no longer needs to know the size of the internal NVMe request structure to create the pool, the <code>spdk_nvme_request_size()</code> function was also removed.</p>
<p>The <code>spdk_nvme_ns_cmd_deallocate()</code> function was renamed and extended to become <code><a class="el" href="nvme_8h.html#ac0c646dd18675c54ffcf834ce699658d" title="Submits a data set management request to the specified NVMe namespace. ">spdk_nvme_ns_cmd_dataset_management()</a></code>, which allows access to all of the NVMe Dataset Management command's parameters. Existing callers can be updated to use <code><a class="el" href="nvme_8h.html#ac0c646dd18675c54ffcf834ce699658d" title="Submits a data set management request to the specified NVMe namespace. ">spdk_nvme_ns_cmd_dataset_management()</a></code> with <code>SPDK_NVME_DSM_ATTR_DEALLOCATE</code> as the <code>type</code> parameter.</p>
<p>The NVMe library SGL callback prototype has been changed to return virtual addresses rather than physical addresses. Callers of <code><a class="el" href="nvme_8h.html#acb47ce7de6b6e963ec9fb8de261466ae" title="Submits a read I/O to the specified NVMe namespace. ">spdk_nvme_ns_cmd_readv()</a></code> and <code><a class="el" href="nvme_8h.html#adfcbb5d31f0b572847cc8ae8b07dfcfb" title="Submits a write I/O to the specified NVMe namespace. ">spdk_nvme_ns_cmd_writev()</a></code> must update their <code>next_sge_fn</code> callbacks to match.</p>
<p>The NVMe library now supports NVMe over Fabrics devices in addition to the existing support for local PCIe-attached NVMe devices. For an example of how to enable NVMe over Fabrics support in an application, see <code>examples/nvme/identify</code> and <code>examples/nvme/perf</code>.</p>
<p>Hot insert/remove support for NVMe devices has been added. To enable NVMe hotplug support, an application should call the <code><a class="el" href="nvme_8h.html#a225bbc386ec518ae21bd5536f21db45d" title="Enumerate the bus indicated by the transport ID and attach the userspace NVMe driver to each device f...">spdk_nvme_probe()</a></code> function on a regular basis to probe for new devices (reported via the existing <code>probe_cb</code> callback) and removed devices (reported via a new <code>remove_cb</code> callback). Hotplug is currently only supported on Linux with the <code>uio_pci_generic</code> driver, and newly-added NVMe devices must be bound to <code>uio_pci_generic</code> by an external script or tool.</p>
<p>Multiple processes may now coordinate and use a single NVMe device simultaneously using <a href="http://dpdk.org/doc/guides/prog_guide/multi_proc_support.html">DPDK Multi-process Support</a>.</p>
<h2>NVMe over Fabrics target (<code>nvmf_tgt</code>)</h2>
<p>The <code>nvmf_tgt</code> configuration file format has been updated significantly to enable new features. See the example configuration file <code>etc/spdk/nvmf.conf.in</code> for more details on the new and changed options.</p>
<p>The NVMe over Fabrics target now supports virtual mode subsystems, which allow the user to export devices from the SPDK block device abstraction layer as NVMe over Fabrics subsystems. Direct mode (raw NVMe device access) is also still supported, and a single <code>nvmf_tgt</code> may export both types of subsystems simultaneously.</p>
<h2>Block device abstraction layer (bdev)</h2>
<p>The bdev layer now supports scatter/gather read and write I/O APIs, and the NVMe blockdev driver has been updated to support scatter/gather. Apps can use the new scatter/gather support via the <code><a class="el" href="bdev_8h.html#a9ac9efa882e87909acfd4bccaddb1778" title="Submit a read request to the bdev on the given channel. ">spdk_bdev_readv()</a></code> and <code><a class="el" href="bdev_8h.html#a9a508a1c301a1321faf0680a8f31f59a" title="Submit a write request to the bdev on the given channel. ">spdk_bdev_writev()</a></code> functions.</p>
<p>The bdev status returned from each I/O has been extended to pass through NVMe or SCSI status codes directly in cases where the underlying device can provide a more specific status code.</p>
<p>A Ceph RBD (RADOS Block Device) blockdev driver has been added. This allows the <code>iscsi_tgt</code> and <code>nvmf_tgt</code> apps to export Ceph RBD volumes as iSCSI LUNs or NVMe namespaces.</p>
<h2>General changes</h2>
<p><code>libpciaccess</code> has been removed as a dependency and DPDK PCI enumeration is used instead. Prior to DPDK 16.07 enumeration by class code was not supported, so for earlier DPDK versions, only Intel SSD DC P3x00 devices will be discovered by the NVMe library.</p>
<p>The <code>env</code> environment abstraction library has been introduced, and a default DPDK-based implementation is provided as part of SPDK. The goal of the <code>env</code> layer is to enable use of alternate user-mode memory allocation and PCI access libraries. See <code>doc/porting.md</code> for more details.</p>
<p>The build process has been modified to produce all of the library files in the <code>build/lib</code> directory. This is intended to simplify the use of SPDK from external projects, which can now link to SPDK libraries by adding the <code>build/lib</code> directory to the library path via <code>-L</code> and linking the SPDK libraries by name (for example, <code>-lspdk_nvme -lspdk_log -lspdk_util</code>).</p>
<p><code>nvmf_tgt</code> and <code>iscsi_tgt</code> now have a JSON-RPC interface, which allows the user to query and modify the configuration at runtime. The RPC service is disabled by default, since it currently does not provide any authentication or security mechanisms; it should only be enabled on systems with controlled user access behind a firewall. An example RPC client implemented in Python is provided in <code>scripts/rpc.py</code>.</p>
<h1><a class="anchor" id="changelog-v16-08"></a>
v16.08: iSCSI target, NVMe over Fabrics maturity</h1>
<p>This release adds a userspace iSCSI target. The iSCSI target is capable of exporting NVMe devices over a network using the iSCSI protocol. The application is located in app/iscsi_tgt and a documented configuration file can be found at etc/spdk/spdk.conf.in.</p>
<p>This release also significantly improves the existing NVMe over Fabrics target.</p><ul>
<li>The configuration file format was changed, which will require updates to any existing nvmf.conf files (see <code>etc/spdk/nvmf.conf.in</code>):<ul>
<li><code>SubsystemGroup</code> was renamed to <code>Subsystem</code>.</li>
<li><code>AuthFile</code> was removed (it was unimplemented).</li>
<li><code>nvmf_tgt</code> was updated to correctly recognize NQN (NVMe Qualified Names) when naming subsystems. The default node name was changed to reflect this; it is now "nqn.2016-06.io.spdk".</li>
<li><code>Port</code> and <code>Host</code> sections were merged into the <code>Subsystem</code> section</li>
<li>Global options to control max queue depth, number of queues, max I/O size, and max in-capsule data size were added.</li>
<li>The Nvme section was removed. Now a list of devices is specified by bus/device/function directly in the Subsystem section.</li>
<li>Subsystems now have a Mode, which can be Direct or Virtual. This is an attempt to future-proof the interface, so the only mode supported by this release is "Direct".</li>
</ul>
</li>
<li>Many bug fixes and cleanups were applied to the <code>nvmf_tgt</code> app and library.</li>
<li>The target now supports discovery.</li>
</ul>
<p>This release also adds one new feature and provides some better examples and tools for the NVMe driver.</p><ul>
<li>The Weighted Round Robin arbitration method is now supported. This allows the user to specify different priorities on a per-I/O-queue basis. To enable WRR, set the <code>arb_mechanism</code> field during <code><a class="el" href="nvme_8h.html#a225bbc386ec518ae21bd5536f21db45d" title="Enumerate the bus indicated by the transport ID and attach the userspace NVMe driver to each device f...">spdk_nvme_probe()</a></code>.</li>
<li>A simplified "Hello World" example was added to show the proper way to use the NVMe library API; see <code>examples/nvme/hello_world/hello_world.c</code>.</li>
<li>A test for measuring software overhead was added. See <code>test/lib/nvme/overhead</code>.</li>
</ul>
<h1><a class="anchor" id="changelog-v16-06"></a>
v16.06: NVMf userspace target</h1>
<p>This release adds a userspace NVMf (NVMe over Fabrics) target, conforming to the newly-released NVMf 1.0/NVMe 1.2.1 specification. The NVMf target exports NVMe devices from a host machine over the network via RDMA. Currently, the target is limited to directly exporting physical NVMe devices, and the discovery subsystem is not supported.</p>
<p>This release includes a general API cleanup, including renaming all declarations in public headers to include a <code>spdk</code> prefix to prevent namespace clashes with user code.</p>
<ul>
<li>NVMe<ul>
<li>The <code>nvme_attach()</code> API was reworked into a new probe/attach model, which moves device detection into the NVMe library. The new API also allows parallel initialization of NVMe controllers, providing a major reduction in startup time when using multiple controllers.</li>
<li>I/O queue allocation was changed to be explicit in the API. Each function that generates I/O requests now takes a queue pair (<code>spdk_nvme_qpair *</code>) argument, and I/O queues may be allocated using <code><a class="el" href="nvme_8h.html#a13f745d239dab9b8f934fae2ad4984a2" title="Allocate an I/O queue pair (submission and completion queue). ">spdk_nvme_ctrlr_alloc_io_qpair()</a></code>. This allows more flexible assignment of queue pairs than the previous model, which only allowed a single queue per thread and limited the total number of I/O queues to the lowest number supported on any attached controller.</li>
<li>Added support for the Write Zeroes command.</li>
<li><code>examples/nvme/perf</code> can now report I/O command latency from the the controller's viewpoint using the Intel vendor-specific read/write latency log page.</li>
<li>Added namespace reservation command support, which can be used to coordinate sharing of a namespace between multiple hosts.</li>
<li>Added hardware SGL support, which enables use of scattered buffers that don't conform to the PRP list alignment and length requirements on supported NVMe controllers.</li>
<li>Added end-to-end data protection support, including the ability to write and read metadata in extended LBA (metadata appended to each block of data in the buffer) and separate metadata buffer modes. See <code><a class="el" href="nvme_8h.html#adc2aa2be0d657be0c63d5abc02b274ec" title="Submits a write I/O to the specified NVMe namespace. ">spdk_nvme_ns_cmd_write_with_md()</a></code> and <code><a class="el" href="nvme_8h.html#aa2913b93326e636eca6dfe7b42e349fe" title="Submits a read I/O to the specified NVMe namespace. ">spdk_nvme_ns_cmd_read_with_md()</a></code> for details.</li>
</ul>
</li>
<li>IOAT<ul>
<li>The DMA block fill feature is now exposed via the <code>ioat_submit_fill()</code> function. This is functionally similar to <code>memset()</code>, except the memory is filled with an 8-byte repeating pattern instead of a single byte like memset.</li>
</ul>
</li>
<li>PCI<ul>
<li>Added support for using DPDK for PCI device mapping in addition to the existing libpciaccess option. Using the DPDK PCI support also allows use of the Linux VFIO driver model, which means that SPDK userspace drivers will work with the IOMMU enabled. Additionally, SPDK applications may be run as an unprivileged user with access restricted to a specific set of PCIe devices.</li>
<li>The PCI library API was made more generic to abstract away differences between the underlying PCI access implementations.</li>
</ul>
</li>
</ul>
<h1>v1.2.0: IOAT user-space driver</h1>
<p>This release adds a user-space driver with support for the Intel I/O Acceleration Technology (I/OAT, also known as "Crystal Beach") DMA offload engine.</p>
<ul>
<li>IOAT<ul>
<li>New user-space driver supporting DMA memory copy offload</li>
<li>Example programs <code>ioat/perf</code> and <code>ioat/verify</code></li>
<li>Kernel-mode DMA engine test driver <code>kperf</code> for performance comparison</li>
</ul>
</li>
<li>NVMe<ul>
<li>Per-I/O flags for Force Unit Access (FUA) and Limited Retry</li>
<li>Public API for retrieving log pages</li>
<li>Reservation register/acquire/release/report command support</li>
<li>Scattered payload support - an alternate API to provide I/O buffers via a sequence of callbacks</li>
<li>Declarations and <code>nvme/identify</code> support for Intel SSD DC P3700 series vendor-specific log pages and features</li>
</ul>
</li>
<li>Updated to support DPDK 2.2.0</li>
</ul>
<h1>v1.0.0: NVMe user-space driver</h1>
<p>This is the initial open source release of the Storage Performance Development Kit (SPDK).</p>
<p>Features:</p><ul>
<li>NVMe user-space driver</li>
<li>NVMe example programs<ul>
<li><code>examples/nvme/perf</code> tests performance (IOPS) using the NVMe user-space driver</li>
<li><code>examples/nvme/identify</code> displays NVMe controller information in a human-readable format</li>
</ul>
</li>
<li>Linux and FreeBSD support </li>
</ul>
</div></div><!-- contents -->
<footer>
  <div class="container text-center">
    <p class="copyright text-muted small">Copyright © Intel Corporation. All Rights Reserved.</p>
  </div>
</footer>
</div>
