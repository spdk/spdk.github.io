<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <!-- For Mobile Devices -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/xhtml; charset=utf-8">
  <meta name="generator" content="Doxygen 1.8.11">
  <title>SPDK: vhost</title>
  <script type="text/javascript" src="jquery.js"></script>
  <script type="text/javascript" src="dynsections.js"></script>
  <link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,900" type="text/css">
  <link href="../css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
  <link rel="stylesheet" href="tabs.css" type="text/css">
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div class="container-fluid">
  <div id="top">  <!-- do not remove this div, it is closed by doxygen! -->
    <div class="row no-gutters">
      <div class="col-sm-12">
        <section id="nav">
          <div class="navbar navbar-default navbar-static-top banner-tabs">
            <ul class="nav navbar-nav">
              <li role="presentation">
                <a href="http://www.spdk.io/">
                  <i class="glyphicon glyphicon-home"></i>
                  <span class="box-name">home</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/releases/">
                  <i class="glyphicon glyphicon-download-alt"></i>
                  <span class="box-name">download</span>
                </a>
              </li>
              <li class="active" role="presentation">
                <a href="index.html">
                  <i class="glyphicon glyphicon-book"></i>
                  <span class="box-name">documentation</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/development/">
                  <i class="glyphicon glyphicon-wrench"></i>
                  <span class="box-name">development</span>
                </a>
              </li>
              <li role="presentation">
                <a href="https://ci.spdk.io/">
                  <i class="glyphicon glyphicon-ok"></i>
                  <span class="box-name">CI status</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/community/">
                  <i class="glyphicon glyphicon-envelope"></i>
                  <span class="box-name">community</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/blog/">
                  <i class="glyphicon glyphicon-comment"></i>
                  <span class="box-name">Blog</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/roadmap/">
                  <i class="glyphicon glyphicon-map-marker"></i>
                  <span class="box-name">Roadmap</span>
                </a>
              </li>
              <li role="presentation">
                <a href="http://www.spdk.io/news/">
                  <i class="glyphicon glyphicon-bullhorn"></i>
                  <span class="box-name">News</span>
                </a>
              </li>
            </ul>
          </div>
        </section>
      </div>
    </div>
<!-- Generated by Doxygen 1.8.11 -->
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('vhost.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">vhost </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="vhost_users_guide"></a>
vhost Users Guide</h1>
<p>The Storage Performance Development Kit vhost application is named <code>vhost</code>. This application extends SPDK to present virtio storage controllers to QEMU-based VMs and process I/O submitted to devices attached to those controllers.</p>
<h1><a class="anchor" id="vhost_prereqs"></a>
Prerequisites</h1>
<p>This guide assumes the SPDK has been built according to the instructions in <a class="el" href="getting_started.html">Getting Started</a>. The SPDK vhost target is built with the default configure options.</p>
<h2>Supported Guest Operating Systems</h2>
<p>The guest OS must contain virtio-scsi or virtio-blk drivers. Most Linux and FreeBSD distributions include virtio drivers. <a href="https://fedoraproject.org/wiki/Windows_Virtio_Drivers">Windows virtio drivers</a> must be installed separately. The SPDK vhost target has been tested with Ubuntu 16.04, Fedora 25, and Windows 2012 R2.</p>
<h2>QEMU</h2>
<p>Userspace vhost-scsi target support was added to upstream QEMU in v2.10.0. Run the following command to confirm your QEMU supports userspace vhost-scsi.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;qemu-system-x86_64 -device vhost-user-scsi-pci,help</div></div><!-- fragment --><p>Userspace vhost-blk target support is not yet upstream in QEMU, but patches are available in SPDK's QEMU repository:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;git clone -b spdk https://github.com/spdk/qemu</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;cd qemu</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;mkdir build</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;cd build</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;../configure</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;make</div></div><!-- fragment --><h1><a class="anchor" id="vhost_start"></a>
Starting SPDK vhost target</h1>
<p>First, run the SPDK setup.sh script to setup some hugepages for the SPDK vhost target application. This will allocate 4096MiB (4GiB) of hugepages, enough for the SPDK vhost target and the virtual machine.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;HUGEMEM=4096 scripts/setup.sh</div></div><!-- fragment --><p>Next, start the SPDK vhost target application. The following command will start vhost on CPU cores 0 and 1 (cpumask 0x3) with all future socket files placed in /var/tmp. Vhost will fully occupy given CPU cores for I/O polling. Particular vhost devices can be yet restricted to run on a subset of these CPU cores. See <a class="el" href="vhost.html#vhost_dev_create">Create a virtio device</a> for details.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;app/vhost/vhost -S /var/tmp -m 0x3</div></div><!-- fragment --><h1><a class="anchor" id="vhost_config"></a>
SPDK Configuration</h1>
<h2><a class="anchor" id="vhost_bdev_create"></a>
Create bdev (block device)</h2>
<p>SPDK bdevs are block devices which will be exposed to the guest OS. For vhost-scsi, bdevs are exposed as as SCSI LUNs on SCSI devices attached to the vhost-scsi controller in the guest OS. For vhost-blk, bdevs are exposed directly as block devices in the guest OS and are not associated at all with SCSI.</p>
<p>SPDK supports several different types of storage backends, including NVMe, Linux AIO, malloc ramdisk and Ceph RBD. Refer to <a class="el" href="bdev.html#bdev_getting_started">SPDK bdev Getting Started Guide</a> for additional information on configuring SPDK storage backends.</p>
<p>This guide will base on a malloc (ramdisk) bdev named Malloc0. The following RPC will create a 64MB malloc bdev with 512-byte block size.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py construct_malloc_bdev 64 512 Malloc0</div></div><!-- fragment --><h2><a class="anchor" id="vhost_dev_create"></a>
Create a virtio device</h2>
<h3>Vhost-SCSI</h3>
<p>The following RPC will create a vhost-scsi controller which can be accessed by QEMU via /var/tmp/vhost.0. All the I/O polling will be pinned to the least occupied CPU core within given cpumask - in this case always CPU 0. For NUMA systems, the cpumask should specify cores on the same CPU socket as its associated VM.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py construct_vhost_scsi_controller --cpumask 0x1 vhost.0</div></div><!-- fragment --><p>The following RPC will attach the Malloc0 bdev to the vhost.0 vhost-scsi controller. Malloc0 will appear as a single LUN on a SCSI device with target ID 0. SPDK Vhost-SCSI device currently supports only one LUN per SCSI target.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py add_vhost_scsi_lun vhost.0 0 Malloc0</div></div><!-- fragment --><p>To remove a bdev from a vhost-scsi controller use the following RPC:</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py remove_vhost_scsi_dev vhost.0 0</div></div><!-- fragment --><h3>Vhost-BLK</h3>
<p>The following RPC will create a vhost-blk device exposing Malloc0 bdev. The device will be accessible to QEMU via /var/tmp/vhost.1. All the I/O polling will be pinned to the least occupied CPU core within given cpumask - in this case always CPU 0. For NUMA systems, the cpumask should specify cores on the same CPU socket as its associated VM.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py construct_vhost_blk_controller --cpumask 0x1 vhost.1 Malloc0</div></div><!-- fragment --><p>It is also possible to construct a read-only vhost-blk device by specifying an extra <code>-r</code> or <code>--readonly</code> parameter.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py construct_vhost_blk_controller --cpumask 0x1 -r vhost.1 Malloc0</div></div><!-- fragment --><h2><a class="anchor" id="vhost_qemu_config"></a>
QEMU</h2>
<p>Now the virtual machine can be started with QEMU. The following command-line parameters must be added to connect the virtual machine to its vhost controller.</p>
<p>First, specify the memory backend for the virtual machine. Since QEMU must share the virtual machine's memory with the SPDK vhost target, the memory must be specified in this format with share=on.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;-object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on</div></div><!-- fragment --><p>Second, ensure QEMU boots from the virtual machine image and not the SPDK malloc block device by specifying bootindex=0 for the boot image.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;-drive file=guest_os_image.qcow2,if=none,id=disk</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;-device ide-hd,drive=disk,bootindex=0</div></div><!-- fragment --><p>Finally, specify the SPDK vhost devices:</p>
<h3>Vhost-SCSI</h3>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;-chardev socket,id=char0,path=/var/tmp/vhost.0</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;-device vhost-user-scsi-pci,id=scsi0,chardev=char0</div></div><!-- fragment --><h3>Vhost-BLK</h3>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;-chardev socket,id=char1,path=/var/tmp/vhost.1</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;-device vhost-user-blk-pci,id=blk0,chardev=char1,logical_block_size=512,size=64M</div></div><!-- fragment --><h2><a class="anchor" id="vhost_example"></a>
Example output</h2>
<p>TODO add actual outputs</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# HUGENODE=0 HUGEMEM=2048 ./scripts/setup.sh</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;0000:01:00.0 (8086 0953): nvme -&gt; vfio-pci</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;0000:02:00.0 (8086 0953): nvme -&gt; vfio-pci</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# ./app/vhost/vhost -s 1024 -m 0x3 &amp;</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;&lt;TODO log&gt;</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# ./scripts/rpc.py construct_nvme_bdev -b Nvme0 -t pcie -a 0000:01:00.0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# ./scripts/rpc.py construct_nvme_bdev -b Nvme1 -t pcie -a 0000:02:00.0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# ./scripts/rpc.py construct_vhost_scsi_controller --cpumask 0x1 vhost.0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# ./scripts/rpc.py add_vhost_scsi_lun vhost.0 0 Nvme0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# ./scripts/rpc.py add_vhost_scsi_lun vhost.0 0 Nvme0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py construct_malloc_bdev 64 512 Malloc0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py construct_vhost_blk_controller --cpumask 0x2 vhost.1 Malloc0</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# taskset -c 3,4 qemu-system-x86_64 \</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;  --enable-kvm \</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;  -cpu host -smp 2 \</div><div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;  -m 1G -object memory-backend-file,id=mem0,size=1G,mem-path=/dev/hugepages,share=on -numa node,memdev=mem0 \</div><div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;  -drive file=guest_os_image.qcow2,if=none,id=disk \</div><div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;  -device ide-hd,drive=disk,bootindex=0 \</div><div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;  -chardev socket,id=spdk_vhost_scsi0,path=/tmp/vhost.0 \</div><div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;  -device vhost-user-scsi-pci,id=scsi0,chardev=spdk_vhost_scsi0,num_queues=4 \</div><div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;  -chardev socket,id=spdk_vhost_blk0,path=/tmp/vhost.1 \</div><div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;  -device vhost-user-blk-pci,logical_block_size=512,size=64M,chardev=spdk_vhost_blk0,num_queues=4</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;guest:~# lsblk --output &quot;NAME,KNAME,MODEL,HCTL,SIZE,VENDOR,SUBSYSTEMS&quot;</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;NAME   KNAME MODEL            HCTL        SIZE VENDOR   SUBSYSTEMS</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;&lt;TODO log&gt;</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;guest:~# poweroff</div></div><!-- fragment --><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;host:~# fg</div><div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;&lt;&lt; CTRL + C &gt;&gt;</div><div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;vhost.c:1006:session_shutdown: *NOTICE*: Exiting</div></div><!-- fragment --><p>We can see that <code>sdb</code> and <code>sdc</code> are SPDK vhost-scsi LUNs, and <code>vda</code> is SPDK a vhost-blk disk.</p>
<h1>Advanced Topics</h1>
<h2><a class="anchor" id="vhost_multiqueue"></a>
Multi-Queue Block Layer (blk-mq)</h2>
<p>For best performance use the Linux kernel block multi-queue feature with vhost. To enable it on Linux, it is required to modify kernel options inside the virtual machine.</p>
<p>Instructions below for Ubuntu OS:</p><ol type="1">
<li><code>vi /etc/default/grub</code></li>
<li>Make sure mq is enabled: <code>GRUB_CMDLINE_LINUX="scsi_mod.use_blk_mq=1"</code></li>
<li><code>sudo update-grub</code></li>
<li>Reboot virtual machine</li>
</ol>
<p>To achieve better performance, make sure to increase number of cores assigned to the VM and add <code>num_queues</code> parameter to the QEMU <code>device</code>. It should be enough to set <code>num_queues=4</code> to saturate physical device. Adding too many queues might lead to SPDK vhost performance degradation if many vhost devices are used because each device will require additional <code>num_queues</code> to be polled.</p>
<h2><a class="anchor" id="vhost_hotattach"></a>
Hot-attach/hot-detach</h2>
<p>Hotplug/hotremove within a vhost controller is called hot-attach/detach. This is to distinguish it from SPDK bdev hotplug/hotremove. E.g. if an NVMe bdev is attached to a vhost-scsi controller, physically hotremoving the NVMe will trigger a vhost-scsi hot-detach. It is also possible to hot-detach a bdev manually via RPC - for example when the bdev is about to be attached to another controller. See the details below.</p>
<p>Please also note that hot-attach/detach is Vhost-SCSI-specific. There are no RPCs to hot-attach/detach the bdev from a Vhost-BLK device. If a Vhost-BLK device exposes an NVMe bdev that is hotremoved, all the I/O traffic on that Vhost-BLK device will be aborted - possibly flooding a VM with syslog warnings and errors.</p>
<h3>Hot-attach</h3>
<p>Hot-attach is is done by simply attaching a bdev to a vhost controller with a QEMU VM already started. No other extra action is necessary.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py add_vhost_scsi_lun vhost.0 0 Malloc0</div></div><!-- fragment --><h3>Hot-detach</h3>
<p>Just like hot-attach, the hot-detach is done by simply removing a bdev from a controller when a QEMU VM is already started.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py remove_vhost_scsi_dev vhost.0 0</div></div><!-- fragment --><p>Removing an entire bdev will hot-detach it from a controller as well.</p>
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;scripts/rpc.py delete_bdev Malloc0</div></div><!-- fragment --><h1><a class="anchor" id="vhost_bugs"></a>
Known bugs and limitations</h1>
<h2>Windows virtio-blk driver before version 0.1.130-1 only works with 512-byte sectors</h2>
<p>The Windows <code>viostor</code> driver before version 0.1.130-1 is buggy and does not correctly support vhost-blk devices with non-512-byte block size. See the <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1411092">bug report</a> for more information. </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
</div>
